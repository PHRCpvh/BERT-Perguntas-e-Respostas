{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Perguntas e Respostas.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZljPBjOmEpbTK6BgJoqIK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PHRCpvh/BERT-Perguntas-e-Respostas/blob/master/BERT_Perguntas_e_Respostas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76_nhaun2lPF",
        "colab_type": "text"
      },
      "source": [
        "# 1 - Instalação das Bibliotecas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEBBpcWQNPtP",
        "colab_type": "text"
      },
      "source": [
        "• SQUAD Link: https://rajpurkar.github.io/SQuAD-explorer/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXmJ8RktNNAR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f27e7e50-ebd2-4182-e47b-d0bdfc12ee51"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install tf-models-nightly\n",
        "!pip install tf-nightly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 3.3MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n",
            "Collecting tf-models-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/84/41fd1a4d04ac996981d8ff6fdcfa41cf3ab7ca21c130df0d8351f9068daf/tf_models_nightly-2.3.0.dev20200916-py2.py3-none-any.whl (996kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 3.4MB/s \n",
            "\u001b[?25hCollecting opencv-python-headless\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/2a/496e06fd289c01dc21b11970be1261c87ce1cc22d5340c14b516160822a7/opencv_python_headless-4.4.0.42-cp36-cp36m-manylinux2014_x86_64.whl (36.6MB)\n",
            "\u001b[K     |████████████████████████████████| 36.6MB 113kB/s \n",
            "\u001b[?25hCollecting py-cpuinfo>=3.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/f5/8e6e85ce2e9f6e05040cf0d4e26f43a4718bcc4bce988b433276d4b1a5c1/py-cpuinfo-7.0.0.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.9.0)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.8.3)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.29.21)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.18.5)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (5.4.8)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.4.1)\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 44.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.7.12)\n",
            "Collecting tf-slim>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 44.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (7.0.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (2.0.2)\n",
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/fd/42a8e02f4b4a1d2aad7f48fa3d4a33966f5ae8ec929555738d30d5752547/tf_nightly-2.4.0.dev20200916-cp36-cp36m-manylinux2010_x86_64.whl (390.1MB)\n",
            "\u001b[K     |████████████████████████████████| 390.1MB 40kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (3.2.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.1.91)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.0.5)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.5.8)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.21.0)\n",
            "Collecting tensorflow-model-optimization>=0.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/38/4fd48ea1bfcb0b6e36d949025200426fe9c3a8bfae029f0973d85518fa5a/tensorflow_model_optimization-0.5.0-py2.py3-none-any.whl (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 48.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.7)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.3.0)\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (4.1.3)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (2.1.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub>=0.6.0->tf-models-nightly) (3.12.4)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->tf-models-nightly) (2.7.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (1.17.2)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (0.17.4)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf-slim>=1.1.0->tf-models-nightly) (0.10.0)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools->tf-models-nightly) (50.3.0)\n",
            "Collecting flatbuffers>=1.12\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (2.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.32.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.2.0)\n",
            "Collecting tb-nightly<3.0.0a0,>=2.4.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/75/782953ff376eca2396975b47492a5ecb8ae6e86101eab7ba7ee43b1aa388/tb_nightly-2.4.0a20200916-py3-none-any.whl (10.2MB)\n",
            "\u001b[K     |████████████████████████████████| 10.2MB 42.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.3.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.12.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (3.3.0)\n",
            "Collecting tf-estimator-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/7a/3e967083e2fdc37d72d48262c623bbde3078c274ff83073b2cd8dd0f8c75/tf_estimator_nightly-2.4.0.dev2020091601-py2.py3-none-any.whl (460kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 45.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.35.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->tf-models-nightly) (2018.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (4.41.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (2020.6.20)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (4.0.1)\n",
            "Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (0.0.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (1.24.3)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-nightly) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-nightly) (0.4.1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-nightly) (0.1.5)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->tf-models-nightly) (2.4.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-nightly) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-nightly) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-nightly) (4.6)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (20.2.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.3.2)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.24.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.16.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-nightly) (4.1.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly->tf-models-nightly) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly->tf-models-nightly) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly->tf-models-nightly) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly->tf-models-nightly) (1.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle>=1.3.9->tf-models-nightly) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle>=1.3.9->tf-models-nightly) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-nightly) (1.3)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-bigquery>=0.31.0->tf-models-nightly) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->tf-models-nightly) (1.52.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly->tf-models-nightly) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly->tf-models-nightly) (1.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly->tf-models-nightly) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly->tf-models-nightly) (3.1.0)\n",
            "Building wheels for collected packages: py-cpuinfo, pyyaml, seqeval\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-7.0.0-cp36-none-any.whl size=20071 sha256=80b61de908a1dfe587fa7fedb9f0487295a3a385eb5d616157e8e91b4045f882\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/93/7b/127daf0c3a5a49feb2fecd468d508067c733fba5192f726ad1\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=df7c9250daf602d8d62bb907c281338b8fda5b510740e128d5cfd62b56c26163\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7423 sha256=fed9780a2274de9c6911430973d8a6278f558317dc3667c15d889209b123dde1\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built py-cpuinfo pyyaml seqeval\n",
            "Installing collected packages: opencv-python-headless, py-cpuinfo, pyyaml, tf-slim, flatbuffers, tb-nightly, tf-estimator-nightly, tf-nightly, tensorflow-model-optimization, seqeval, tf-models-nightly\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed flatbuffers-1.12 opencv-python-headless-4.4.0.42 py-cpuinfo-7.0.0 pyyaml-5.3.1 seqeval-0.0.12 tb-nightly-2.4.0a20200916 tensorflow-model-optimization-0.5.0 tf-estimator-nightly-2.4.0.dev2020091601 tf-models-nightly-2.3.0.dev20200916 tf-nightly-2.4.0.dev20200916 tf-slim-1.1.0\n",
            "Requirement already satisfied: tf-nightly in /usr/local/lib/python3.6/dist-packages (2.4.0.dev20200916)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\n",
            "Requirement already satisfied: tf-estimator-nightly in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.4.0.dev2020091601)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: tb-nightly<3.0.0a0,>=2.4.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.4.0a20200916)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.32.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.18.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.35.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.10.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tf-nightly) (50.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.7.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.17.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Bu1itANOEao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!#pip install tensorflow==2.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISWKGUQKN62l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a9bca9da-280d-4a3c-e1b3-4777b740bca4"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0-dev20200916'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNGRVwo-Pta6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "from official.nlp.bert.tokenization import FullTokenizer\n",
        "from official.nlp.bert.input_pipeline import create_squad_dataset\n",
        "from official.nlp.data.squad_lib import generate_tf_record_from_json_file\n",
        "from official.nlp import optimization\n",
        "from official.nlp.data.squad_lib import read_squad_examples\n",
        "from official.nlp.data.squad_lib import FeatureWriter\n",
        "from official.nlp.data.squad_lib import convert_examples_to_features\n",
        "from official.nlp.data.squad_lib import write_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDrfif9sQYft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "import collections\n",
        "import os\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPsZoLw_Qkho",
        "colab_type": "text"
      },
      "source": [
        "# 2 - Pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv7EbeEPQn17",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f947b413-3ed0-47ea-fe84-1d979a52538c"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsU0A2KjdE8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_meta_data = generate_tf_record_from_json_file(\n",
        "    \"/content/drive/My Drive/Google Colab/BERT/train-v1.1.json\",\n",
        "    \"/content/drive/My Drive/Google Colab/BERT/vocab.txt\",\n",
        "    \"/content/drive/My Drive/Google Colab/BERT/train-v1.1.tf_record\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO7CiOxtKWyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.io.gfile.GFile(\"/content/drive/My Drive/Google Colab/BERT/train_meta_data\", \"w\") as writer:\n",
        "  writer.write(json.dumps(input_meta_data, indent=4) + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bElbRkLPwPUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "train_dataset = create_squad_dataset(\n",
        "    \"/content/drive/My Drive/Google Colab/BERT/train-v1.1.tf_record\",\n",
        "    input_meta_data[\"max_seq_length\"], # 384\n",
        "    BATCH_SIZE,\n",
        "    is_training = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYvB91Xuw7No",
        "colab_type": "text"
      },
      "source": [
        "# 3 - Construção do Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg67el0w--d",
        "colab_type": "text"
      },
      "source": [
        "Camada SQUAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mKANW6ewxYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class BertSquadLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(BertSquadLayer, self).__init__()\n",
        "    self.final_dense = tf.keras.layers.Dense(units=2,\n",
        "                                             kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    logits = self.final_dense(inputs) # (batch_size, seq_len, 2)\n",
        "    logits = tf.transpose(logits, [2, 0, 1]) # (2, batch_size, seq_len)\n",
        "    unstacked_logits = tf.unstack(logits, axis=0) # [(batch_size, seq_len), (batch_size, seq_len)]\n",
        "    return unstacked_logits[0], unstacked_logits[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIBIkK0SykJm",
        "colab_type": "text"
      },
      "source": [
        "Modelo Completo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMuH7WvEylcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTSquad(tf.keras.Model):\n",
        "  def __init__(self, name=\"bert_squad\"):\n",
        "    super(BERTSquad, self).__init__(name=name)\n",
        "    self.bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                                     trainable = True)\n",
        "    self.squad_layer = BertSquadLayer()\n",
        "\n",
        "  def apply_bert(self, inputs):\n",
        "    _, sequence_output = self.bert_layer([inputs[\"input_word_ids\"],\n",
        "                                          inputs[\"input_mask\"],\n",
        "                                          inputs[\"input_type_ids\"]])\n",
        "    return sequence_output\n",
        "\n",
        "  def call(self, inputs):\n",
        "    seq_outputs = self.apply_bert(inputs)\n",
        "    start_logits, end_logits = self.squad_layer(seq_outputs)\n",
        "    return start_logits, end_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWjc6fa30OOm",
        "colab_type": "text"
      },
      "source": [
        "# 4 - Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxf2CONl0RRa",
        "colab_type": "text"
      },
      "source": [
        "Criação da IA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ-c0z0F005g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_DATA_SIZE = 88641\n",
        "NB_BATCHES_TRAIN = 2000\n",
        "BATCH_SIZE = 4\n",
        "NB_EPOCHS = 3\n",
        "INIT_LR = 5e-5\n",
        "WARMUP_STEPS = int(NB_BATCHES_TRAIN * 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHC55fWs1gvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_light = train_dataset.take(NB_BATCHES_TRAIN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n5QlxoZ1mTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_squad = BERTSquad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpzotZWL1qYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optimization.create_optimizer(init_lr=INIT_LR,\n",
        "                                          num_train_steps = NB_BATCHES_TRAIN,\n",
        "                                          num_warmup_steps = WARMUP_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih04XNyd15gf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squad_loss_fn(labels, model_outputs):\n",
        "  start_positions = labels['start_positions']\n",
        "  end_positions = labels['end_positions']\n",
        "  start_logits, end_logits = model_outputs\n",
        "\n",
        "  start_loss = tf.keras.backend.sparse_categorical_crossentropy(start_positions, start_logits, from_logits=True)\n",
        "  end_loss = tf.keras.backend.sparse_categorical_crossentropy(end_positions, end_logits, from_logits=True)\n",
        "\n",
        "  total_loss = (tf.reduce_mean(start_loss) + tf.reduce_mean(end_loss)) / 2\n",
        "\n",
        "  return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDAY0WQi23jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjYWSAbf277c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_squad.compile(optimizer, squad_loss_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXE1DsYUCT2N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2e933faf-e114-4e5c-ae8a-7aee79b6b530"
      },
      "source": [
        "checkpoint_path = \"/content/drive/My Drive/Google Colab/BERT/Q&A\"\n",
        "ckpt = tf.train.Checkpoint(bert_squad = bert_squad)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep = 1)\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest checkpoint restored!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR22jxeg3EWl",
        "colab_type": "text"
      },
      "source": [
        "Treinamento Personalizado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-v0TC_1k3DIf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "981b3184-7955-4818-a4b3-fd760170dc79"
      },
      "source": [
        "for epoch in range(NB_EPOCHS):\n",
        "  print(\"Start of epoch {}\".format(epoch + 1))\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "\n",
        "  for (batch, (inputs, targets)) in enumerate(train_dataset_light):\n",
        "    with tf.GradientTape() as tape:\n",
        "      model_outputs = bert_squad(inputs)\n",
        "      loss = squad_loss_fn(targets, model_outputs)\n",
        "\n",
        "    gradients = tape.gradient(loss, bert_squad.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, bert_squad.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(\"Epoch {} Batch {} Loss {} (:.4f)\".format(epoch+1, batch, train_loss.result()))\n",
        "\n",
        "    if batch % 500 == 0:\n",
        "      ckpt_save_path = ckpt_manager.save()\n",
        "      print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1, ckpt_save_path))\n",
        "\n",
        "  print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 6.132259368896484 (:.4f)\n",
            "Saving checkpoint for epoch 1 at /content/drive/My Drive/Google Colab/BERT/Q&A/ckpt-9\n",
            "Epoch 1 Batch 50 Loss 5.853891372680664 (:.4f)\n",
            "Epoch 1 Batch 100 Loss 5.3107500076293945 (:.4f)\n",
            "Epoch 1 Batch 150 Loss 4.639137268066406 (:.4f)\n",
            "Epoch 1 Batch 200 Loss 4.116105079650879 (:.4f)\n",
            "Epoch 1 Batch 250 Loss 3.695465564727783 (:.4f)\n",
            "Epoch 1 Batch 300 Loss 3.4021105766296387 (:.4f)\n",
            "Epoch 1 Batch 350 Loss 3.247800350189209 (:.4f)\n",
            "Epoch 1 Batch 400 Loss 3.0461175441741943 (:.4f)\n",
            "Epoch 1 Batch 450 Loss 2.88688325881958 (:.4f)\n",
            "Epoch 1 Batch 500 Loss 2.743670701980591 (:.4f)\n",
            "Saving checkpoint for epoch 1 at /content/drive/My Drive/Google Colab/BERT/Q&A/ckpt-10\n",
            "Epoch 1 Batch 550 Loss 2.629446268081665 (:.4f)\n",
            "Epoch 1 Batch 600 Loss 2.542201042175293 (:.4f)\n",
            "Epoch 1 Batch 650 Loss 2.4675955772399902 (:.4f)\n",
            "Epoch 1 Batch 700 Loss 2.3980979919433594 (:.4f)\n",
            "Epoch 1 Batch 750 Loss 2.328103542327881 (:.4f)\n",
            "Epoch 1 Batch 800 Loss 2.274634838104248 (:.4f)\n",
            "Epoch 1 Batch 850 Loss 2.2368040084838867 (:.4f)\n",
            "Epoch 1 Batch 900 Loss 2.185415506362915 (:.4f)\n",
            "Epoch 1 Batch 950 Loss 2.143580675125122 (:.4f)\n",
            "Epoch 1 Batch 1000 Loss 2.080206871032715 (:.4f)\n",
            "Saving checkpoint for epoch 1 at /content/drive/My Drive/Google Colab/BERT/Q&A/ckpt-11\n",
            "Epoch 1 Batch 1050 Loss 2.0259435176849365 (:.4f)\n",
            "Epoch 1 Batch 1100 Loss 1.9809495210647583 (:.4f)\n",
            "Epoch 1 Batch 1150 Loss 1.9341176748275757 (:.4f)\n",
            "Epoch 1 Batch 1200 Loss 1.8992482423782349 (:.4f)\n",
            "Epoch 1 Batch 1250 Loss 1.8762792348861694 (:.4f)\n",
            "Epoch 1 Batch 1300 Loss 1.8696318864822388 (:.4f)\n",
            "Epoch 1 Batch 1350 Loss 1.8506629467010498 (:.4f)\n",
            "Epoch 1 Batch 1400 Loss 1.82749605178833 (:.4f)\n",
            "Epoch 1 Batch 1450 Loss 1.8067469596862793 (:.4f)\n",
            "Epoch 1 Batch 1500 Loss 1.7806072235107422 (:.4f)\n",
            "Saving checkpoint for epoch 1 at /content/drive/My Drive/Google Colab/BERT/Q&A/ckpt-12\n",
            "Epoch 1 Batch 1550 Loss 1.7612305879592896 (:.4f)\n",
            "Epoch 1 Batch 1600 Loss 1.744429111480713 (:.4f)\n",
            "Epoch 1 Batch 1650 Loss 1.7327814102172852 (:.4f)\n",
            "Epoch 1 Batch 1700 Loss 1.726209044456482 (:.4f)\n",
            "Epoch 1 Batch 1750 Loss 1.711916446685791 (:.4f)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-cf5515a840de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquad_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_squad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_squad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_backward_function_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1275\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return backward._call_flat(  # pylint: disable=protected-access\n\u001b[0;32m-> 1277\u001b[0;31m           processed_args, remapped_captures)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_backward_function_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecorded_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1918\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1920\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1921\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1922\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xpx5AX1G2x7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2e9ddbaa-0fde-4d61-dc50-28200476b751"
      },
      "source": [
        "to_restore = tf.Variable(tf.zeros([5]))\n",
        "print(to_restore.numpy())  # All zeros\n",
        "fake_layer = tf.train.Checkpoint(bias=to_restore)\n",
        "fake_net = tf.train.Checkpoint(l1=fake_layer)\n",
        "new_root = tf.train.Checkpoint(net=fake_net)\n",
        "status = new_root.restore(tf.train.latest_checkpoint(\"/content/drive/My Drive/Google Colab/BERT/Q&A/ckpt-12.data-00000-of-00001\"))\n",
        "print(to_restore.numpy())  # We get the restored value now"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkNwdDglIHAV",
        "colab_type": "text"
      },
      "source": [
        "# 5 - Avaliação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM0lU72BIKny",
        "colab_type": "text"
      },
      "source": [
        "Preparação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTU4az44IMGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_examples = read_squad_examples(\n",
        "    \"/content/drive/My Drive/Google Colab/BERT/dev-v1.1.json\",\n",
        "    is_training = False,\n",
        "    version_2_with_negative = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nL-iGSVLmkd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> Definição da função para gerar o tf_record\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCtsExfcI7aN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_writer = FeatureWriter(\n",
        "    filename=os.path.join(\"/content/drive/My Drive/Google Colab/BERT\",\n",
        "                          \"eval.tf_record\"),\n",
        "    is_training = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaaVPsDvLw2I",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> Tokenizador\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEc8I4L7J1Ba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable = False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9E4R_7_L53W",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> Caracterizador da lista \"eva_features\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NohCNaPfKWAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _append_feature(feature, is_padding):\n",
        "  if not is_padding:\n",
        "      eval_features.append(feature)\n",
        "  eval_writer.process_feature(feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC3xlOpFL_Mu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> Caracterizador do arquivo tf.record\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV6_Hc3aKkSx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_features = []\n",
        "dataset_size = convert_examples_to_features(\n",
        "    examples = eval_examples,\n",
        "    tokenizer = tokenizer,\n",
        "    max_seq_length = 384,\n",
        "    doc_stride = 128,\n",
        "    max_query_length = 64,\n",
        "    is_training = False,\n",
        "    output_fn = _append_feature,\n",
        "    batch_size = 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCVaapQULDQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xpvc8i6METP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> Base de dados\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp8a1LJdLI6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "eval_dataset = create_squad_dataset(\n",
        "    \"/content/drive/My Drive/Google Colab/BERT/eval.tf_record\",\n",
        "    384, # input_meta_data['max_seq_length'],\n",
        "    BATCH_SIZE,\n",
        "    is_training = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj3OPrPwITC3",
        "colab_type": "text"
      },
      "source": [
        "Previsões com funções do Google"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0daNWWlZMOYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RawResult = collections.namedtuple(\"RawResult\",\n",
        "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_3N1rRsMlMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_raw_results(predictions):\n",
        "  for unique_ids, start_logits, end_logits in zip(predictions[\"unique_ids\"],\n",
        "                                                  predictions[\"start_logits\"],\n",
        "                                                  predictions[\"end_logits\"]):\n",
        "    yield RawResult(\n",
        "        unique_id=unique_ids.numpy(),\n",
        "        start_logits=start_logits.numpy().tolist(),\n",
        "        end_logits=end_logits.numpy().tolist()\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbqBz6XbNRjB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8e83780b-8ca6-4a2e-9495-5cbbe07fa07b"
      },
      "source": [
        "all_results = []\n",
        "for count, inputs in enumerate(eval_dataset):\n",
        "  x, _ = inputs\n",
        "  #print(x)\n",
        "  unique_ids = x.pop(\"unique_ids\")\n",
        "  start_logits, end_logits = bert_squad(x, training = False)\n",
        "  output_dict = dict(\n",
        "      unique_ids = unique_ids,\n",
        "      start_logits = start_logits,\n",
        "      end_logits = end_logits\n",
        "  )\n",
        "  for result in get_raw_results(output_dict):\n",
        "    all_results.append(result)\n",
        "\n",
        "  print(count)\n",
        "  if count % 100 == 0:\n",
        "    print(\"{}/{}\".format(count, 2709))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0/2709\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "100/2709\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "200/2709\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "300/2709\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "400/2709\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "500/2709\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "600/2709\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "700/2709\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "800/2709\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "900/2709\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n",
            "1000\n",
            "1000/2709\n",
            "1001\n",
            "1002\n",
            "1003\n",
            "1004\n",
            "1005\n",
            "1006\n",
            "1007\n",
            "1008\n",
            "1009\n",
            "1010\n",
            "1011\n",
            "1012\n",
            "1013\n",
            "1014\n",
            "1015\n",
            "1016\n",
            "1017\n",
            "1018\n",
            "1019\n",
            "1020\n",
            "1021\n",
            "1022\n",
            "1023\n",
            "1024\n",
            "1025\n",
            "1026\n",
            "1027\n",
            "1028\n",
            "1029\n",
            "1030\n",
            "1031\n",
            "1032\n",
            "1033\n",
            "1034\n",
            "1035\n",
            "1036\n",
            "1037\n",
            "1038\n",
            "1039\n",
            "1040\n",
            "1041\n",
            "1042\n",
            "1043\n",
            "1044\n",
            "1045\n",
            "1046\n",
            "1047\n",
            "1048\n",
            "1049\n",
            "1050\n",
            "1051\n",
            "1052\n",
            "1053\n",
            "1054\n",
            "1055\n",
            "1056\n",
            "1057\n",
            "1058\n",
            "1059\n",
            "1060\n",
            "1061\n",
            "1062\n",
            "1063\n",
            "1064\n",
            "1065\n",
            "1066\n",
            "1067\n",
            "1068\n",
            "1069\n",
            "1070\n",
            "1071\n",
            "1072\n",
            "1073\n",
            "1074\n",
            "1075\n",
            "1076\n",
            "1077\n",
            "1078\n",
            "1079\n",
            "1080\n",
            "1081\n",
            "1082\n",
            "1083\n",
            "1084\n",
            "1085\n",
            "1086\n",
            "1087\n",
            "1088\n",
            "1089\n",
            "1090\n",
            "1091\n",
            "1092\n",
            "1093\n",
            "1094\n",
            "1095\n",
            "1096\n",
            "1097\n",
            "1098\n",
            "1099\n",
            "1100\n",
            "1100/2709\n",
            "1101\n",
            "1102\n",
            "1103\n",
            "1104\n",
            "1105\n",
            "1106\n",
            "1107\n",
            "1108\n",
            "1109\n",
            "1110\n",
            "1111\n",
            "1112\n",
            "1113\n",
            "1114\n",
            "1115\n",
            "1116\n",
            "1117\n",
            "1118\n",
            "1119\n",
            "1120\n",
            "1121\n",
            "1122\n",
            "1123\n",
            "1124\n",
            "1125\n",
            "1126\n",
            "1127\n",
            "1128\n",
            "1129\n",
            "1130\n",
            "1131\n",
            "1132\n",
            "1133\n",
            "1134\n",
            "1135\n",
            "1136\n",
            "1137\n",
            "1138\n",
            "1139\n",
            "1140\n",
            "1141\n",
            "1142\n",
            "1143\n",
            "1144\n",
            "1145\n",
            "1146\n",
            "1147\n",
            "1148\n",
            "1149\n",
            "1150\n",
            "1151\n",
            "1152\n",
            "1153\n",
            "1154\n",
            "1155\n",
            "1156\n",
            "1157\n",
            "1158\n",
            "1159\n",
            "1160\n",
            "1161\n",
            "1162\n",
            "1163\n",
            "1164\n",
            "1165\n",
            "1166\n",
            "1167\n",
            "1168\n",
            "1169\n",
            "1170\n",
            "1171\n",
            "1172\n",
            "1173\n",
            "1174\n",
            "1175\n",
            "1176\n",
            "1177\n",
            "1178\n",
            "1179\n",
            "1180\n",
            "1181\n",
            "1182\n",
            "1183\n",
            "1184\n",
            "1185\n",
            "1186\n",
            "1187\n",
            "1188\n",
            "1189\n",
            "1190\n",
            "1191\n",
            "1192\n",
            "1193\n",
            "1194\n",
            "1195\n",
            "1196\n",
            "1197\n",
            "1198\n",
            "1199\n",
            "1200\n",
            "1200/2709\n",
            "1201\n",
            "1202\n",
            "1203\n",
            "1204\n",
            "1205\n",
            "1206\n",
            "1207\n",
            "1208\n",
            "1209\n",
            "1210\n",
            "1211\n",
            "1212\n",
            "1213\n",
            "1214\n",
            "1215\n",
            "1216\n",
            "1217\n",
            "1218\n",
            "1219\n",
            "1220\n",
            "1221\n",
            "1222\n",
            "1223\n",
            "1224\n",
            "1225\n",
            "1226\n",
            "1227\n",
            "1228\n",
            "1229\n",
            "1230\n",
            "1231\n",
            "1232\n",
            "1233\n",
            "1234\n",
            "1235\n",
            "1236\n",
            "1237\n",
            "1238\n",
            "1239\n",
            "1240\n",
            "1241\n",
            "1242\n",
            "1243\n",
            "1244\n",
            "1245\n",
            "1246\n",
            "1247\n",
            "1248\n",
            "1249\n",
            "1250\n",
            "1251\n",
            "1252\n",
            "1253\n",
            "1254\n",
            "1255\n",
            "1256\n",
            "1257\n",
            "1258\n",
            "1259\n",
            "1260\n",
            "1261\n",
            "1262\n",
            "1263\n",
            "1264\n",
            "1265\n",
            "1266\n",
            "1267\n",
            "1268\n",
            "1269\n",
            "1270\n",
            "1271\n",
            "1272\n",
            "1273\n",
            "1274\n",
            "1275\n",
            "1276\n",
            "1277\n",
            "1278\n",
            "1279\n",
            "1280\n",
            "1281\n",
            "1282\n",
            "1283\n",
            "1284\n",
            "1285\n",
            "1286\n",
            "1287\n",
            "1288\n",
            "1289\n",
            "1290\n",
            "1291\n",
            "1292\n",
            "1293\n",
            "1294\n",
            "1295\n",
            "1296\n",
            "1297\n",
            "1298\n",
            "1299\n",
            "1300\n",
            "1300/2709\n",
            "1301\n",
            "1302\n",
            "1303\n",
            "1304\n",
            "1305\n",
            "1306\n",
            "1307\n",
            "1308\n",
            "1309\n",
            "1310\n",
            "1311\n",
            "1312\n",
            "1313\n",
            "1314\n",
            "1315\n",
            "1316\n",
            "1317\n",
            "1318\n",
            "1319\n",
            "1320\n",
            "1321\n",
            "1322\n",
            "1323\n",
            "1324\n",
            "1325\n",
            "1326\n",
            "1327\n",
            "1328\n",
            "1329\n",
            "1330\n",
            "1331\n",
            "1332\n",
            "1333\n",
            "1334\n",
            "1335\n",
            "1336\n",
            "1337\n",
            "1338\n",
            "1339\n",
            "1340\n",
            "1341\n",
            "1342\n",
            "1343\n",
            "1344\n",
            "1345\n",
            "1346\n",
            "1347\n",
            "1348\n",
            "1349\n",
            "1350\n",
            "1351\n",
            "1352\n",
            "1353\n",
            "1354\n",
            "1355\n",
            "1356\n",
            "1357\n",
            "1358\n",
            "1359\n",
            "1360\n",
            "1361\n",
            "1362\n",
            "1363\n",
            "1364\n",
            "1365\n",
            "1366\n",
            "1367\n",
            "1368\n",
            "1369\n",
            "1370\n",
            "1371\n",
            "1372\n",
            "1373\n",
            "1374\n",
            "1375\n",
            "1376\n",
            "1377\n",
            "1378\n",
            "1379\n",
            "1380\n",
            "1381\n",
            "1382\n",
            "1383\n",
            "1384\n",
            "1385\n",
            "1386\n",
            "1387\n",
            "1388\n",
            "1389\n",
            "1390\n",
            "1391\n",
            "1392\n",
            "1393\n",
            "1394\n",
            "1395\n",
            "1396\n",
            "1397\n",
            "1398\n",
            "1399\n",
            "1400\n",
            "1400/2709\n",
            "1401\n",
            "1402\n",
            "1403\n",
            "1404\n",
            "1405\n",
            "1406\n",
            "1407\n",
            "1408\n",
            "1409\n",
            "1410\n",
            "1411\n",
            "1412\n",
            "1413\n",
            "1414\n",
            "1415\n",
            "1416\n",
            "1417\n",
            "1418\n",
            "1419\n",
            "1420\n",
            "1421\n",
            "1422\n",
            "1423\n",
            "1424\n",
            "1425\n",
            "1426\n",
            "1427\n",
            "1428\n",
            "1429\n",
            "1430\n",
            "1431\n",
            "1432\n",
            "1433\n",
            "1434\n",
            "1435\n",
            "1436\n",
            "1437\n",
            "1438\n",
            "1439\n",
            "1440\n",
            "1441\n",
            "1442\n",
            "1443\n",
            "1444\n",
            "1445\n",
            "1446\n",
            "1447\n",
            "1448\n",
            "1449\n",
            "1450\n",
            "1451\n",
            "1452\n",
            "1453\n",
            "1454\n",
            "1455\n",
            "1456\n",
            "1457\n",
            "1458\n",
            "1459\n",
            "1460\n",
            "1461\n",
            "1462\n",
            "1463\n",
            "1464\n",
            "1465\n",
            "1466\n",
            "1467\n",
            "1468\n",
            "1469\n",
            "1470\n",
            "1471\n",
            "1472\n",
            "1473\n",
            "1474\n",
            "1475\n",
            "1476\n",
            "1477\n",
            "1478\n",
            "1479\n",
            "1480\n",
            "1481\n",
            "1482\n",
            "1483\n",
            "1484\n",
            "1485\n",
            "1486\n",
            "1487\n",
            "1488\n",
            "1489\n",
            "1490\n",
            "1491\n",
            "1492\n",
            "1493\n",
            "1494\n",
            "1495\n",
            "1496\n",
            "1497\n",
            "1498\n",
            "1499\n",
            "1500\n",
            "1500/2709\n",
            "1501\n",
            "1502\n",
            "1503\n",
            "1504\n",
            "1505\n",
            "1506\n",
            "1507\n",
            "1508\n",
            "1509\n",
            "1510\n",
            "1511\n",
            "1512\n",
            "1513\n",
            "1514\n",
            "1515\n",
            "1516\n",
            "1517\n",
            "1518\n",
            "1519\n",
            "1520\n",
            "1521\n",
            "1522\n",
            "1523\n",
            "1524\n",
            "1525\n",
            "1526\n",
            "1527\n",
            "1528\n",
            "1529\n",
            "1530\n",
            "1531\n",
            "1532\n",
            "1533\n",
            "1534\n",
            "1535\n",
            "1536\n",
            "1537\n",
            "1538\n",
            "1539\n",
            "1540\n",
            "1541\n",
            "1542\n",
            "1543\n",
            "1544\n",
            "1545\n",
            "1546\n",
            "1547\n",
            "1548\n",
            "1549\n",
            "1550\n",
            "1551\n",
            "1552\n",
            "1553\n",
            "1554\n",
            "1555\n",
            "1556\n",
            "1557\n",
            "1558\n",
            "1559\n",
            "1560\n",
            "1561\n",
            "1562\n",
            "1563\n",
            "1564\n",
            "1565\n",
            "1566\n",
            "1567\n",
            "1568\n",
            "1569\n",
            "1570\n",
            "1571\n",
            "1572\n",
            "1573\n",
            "1574\n",
            "1575\n",
            "1576\n",
            "1577\n",
            "1578\n",
            "1579\n",
            "1580\n",
            "1581\n",
            "1582\n",
            "1583\n",
            "1584\n",
            "1585\n",
            "1586\n",
            "1587\n",
            "1588\n",
            "1589\n",
            "1590\n",
            "1591\n",
            "1592\n",
            "1593\n",
            "1594\n",
            "1595\n",
            "1596\n",
            "1597\n",
            "1598\n",
            "1599\n",
            "1600\n",
            "1600/2709\n",
            "1601\n",
            "1602\n",
            "1603\n",
            "1604\n",
            "1605\n",
            "1606\n",
            "1607\n",
            "1608\n",
            "1609\n",
            "1610\n",
            "1611\n",
            "1612\n",
            "1613\n",
            "1614\n",
            "1615\n",
            "1616\n",
            "1617\n",
            "1618\n",
            "1619\n",
            "1620\n",
            "1621\n",
            "1622\n",
            "1623\n",
            "1624\n",
            "1625\n",
            "1626\n",
            "1627\n",
            "1628\n",
            "1629\n",
            "1630\n",
            "1631\n",
            "1632\n",
            "1633\n",
            "1634\n",
            "1635\n",
            "1636\n",
            "1637\n",
            "1638\n",
            "1639\n",
            "1640\n",
            "1641\n",
            "1642\n",
            "1643\n",
            "1644\n",
            "1645\n",
            "1646\n",
            "1647\n",
            "1648\n",
            "1649\n",
            "1650\n",
            "1651\n",
            "1652\n",
            "1653\n",
            "1654\n",
            "1655\n",
            "1656\n",
            "1657\n",
            "1658\n",
            "1659\n",
            "1660\n",
            "1661\n",
            "1662\n",
            "1663\n",
            "1664\n",
            "1665\n",
            "1666\n",
            "1667\n",
            "1668\n",
            "1669\n",
            "1670\n",
            "1671\n",
            "1672\n",
            "1673\n",
            "1674\n",
            "1675\n",
            "1676\n",
            "1677\n",
            "1678\n",
            "1679\n",
            "1680\n",
            "1681\n",
            "1682\n",
            "1683\n",
            "1684\n",
            "1685\n",
            "1686\n",
            "1687\n",
            "1688\n",
            "1689\n",
            "1690\n",
            "1691\n",
            "1692\n",
            "1693\n",
            "1694\n",
            "1695\n",
            "1696\n",
            "1697\n",
            "1698\n",
            "1699\n",
            "1700\n",
            "1700/2709\n",
            "1701\n",
            "1702\n",
            "1703\n",
            "1704\n",
            "1705\n",
            "1706\n",
            "1707\n",
            "1708\n",
            "1709\n",
            "1710\n",
            "1711\n",
            "1712\n",
            "1713\n",
            "1714\n",
            "1715\n",
            "1716\n",
            "1717\n",
            "1718\n",
            "1719\n",
            "1720\n",
            "1721\n",
            "1722\n",
            "1723\n",
            "1724\n",
            "1725\n",
            "1726\n",
            "1727\n",
            "1728\n",
            "1729\n",
            "1730\n",
            "1731\n",
            "1732\n",
            "1733\n",
            "1734\n",
            "1735\n",
            "1736\n",
            "1737\n",
            "1738\n",
            "1739\n",
            "1740\n",
            "1741\n",
            "1742\n",
            "1743\n",
            "1744\n",
            "1745\n",
            "1746\n",
            "1747\n",
            "1748\n",
            "1749\n",
            "1750\n",
            "1751\n",
            "1752\n",
            "1753\n",
            "1754\n",
            "1755\n",
            "1756\n",
            "1757\n",
            "1758\n",
            "1759\n",
            "1760\n",
            "1761\n",
            "1762\n",
            "1763\n",
            "1764\n",
            "1765\n",
            "1766\n",
            "1767\n",
            "1768\n",
            "1769\n",
            "1770\n",
            "1771\n",
            "1772\n",
            "1773\n",
            "1774\n",
            "1775\n",
            "1776\n",
            "1777\n",
            "1778\n",
            "1779\n",
            "1780\n",
            "1781\n",
            "1782\n",
            "1783\n",
            "1784\n",
            "1785\n",
            "1786\n",
            "1787\n",
            "1788\n",
            "1789\n",
            "1790\n",
            "1791\n",
            "1792\n",
            "1793\n",
            "1794\n",
            "1795\n",
            "1796\n",
            "1797\n",
            "1798\n",
            "1799\n",
            "1800\n",
            "1800/2709\n",
            "1801\n",
            "1802\n",
            "1803\n",
            "1804\n",
            "1805\n",
            "1806\n",
            "1807\n",
            "1808\n",
            "1809\n",
            "1810\n",
            "1811\n",
            "1812\n",
            "1813\n",
            "1814\n",
            "1815\n",
            "1816\n",
            "1817\n",
            "1818\n",
            "1819\n",
            "1820\n",
            "1821\n",
            "1822\n",
            "1823\n",
            "1824\n",
            "1825\n",
            "1826\n",
            "1827\n",
            "1828\n",
            "1829\n",
            "1830\n",
            "1831\n",
            "1832\n",
            "1833\n",
            "1834\n",
            "1835\n",
            "1836\n",
            "1837\n",
            "1838\n",
            "1839\n",
            "1840\n",
            "1841\n",
            "1842\n",
            "1843\n",
            "1844\n",
            "1845\n",
            "1846\n",
            "1847\n",
            "1848\n",
            "1849\n",
            "1850\n",
            "1851\n",
            "1852\n",
            "1853\n",
            "1854\n",
            "1855\n",
            "1856\n",
            "1857\n",
            "1858\n",
            "1859\n",
            "1860\n",
            "1861\n",
            "1862\n",
            "1863\n",
            "1864\n",
            "1865\n",
            "1866\n",
            "1867\n",
            "1868\n",
            "1869\n",
            "1870\n",
            "1871\n",
            "1872\n",
            "1873\n",
            "1874\n",
            "1875\n",
            "1876\n",
            "1877\n",
            "1878\n",
            "1879\n",
            "1880\n",
            "1881\n",
            "1882\n",
            "1883\n",
            "1884\n",
            "1885\n",
            "1886\n",
            "1887\n",
            "1888\n",
            "1889\n",
            "1890\n",
            "1891\n",
            "1892\n",
            "1893\n",
            "1894\n",
            "1895\n",
            "1896\n",
            "1897\n",
            "1898\n",
            "1899\n",
            "1900\n",
            "1900/2709\n",
            "1901\n",
            "1902\n",
            "1903\n",
            "1904\n",
            "1905\n",
            "1906\n",
            "1907\n",
            "1908\n",
            "1909\n",
            "1910\n",
            "1911\n",
            "1912\n",
            "1913\n",
            "1914\n",
            "1915\n",
            "1916\n",
            "1917\n",
            "1918\n",
            "1919\n",
            "1920\n",
            "1921\n",
            "1922\n",
            "1923\n",
            "1924\n",
            "1925\n",
            "1926\n",
            "1927\n",
            "1928\n",
            "1929\n",
            "1930\n",
            "1931\n",
            "1932\n",
            "1933\n",
            "1934\n",
            "1935\n",
            "1936\n",
            "1937\n",
            "1938\n",
            "1939\n",
            "1940\n",
            "1941\n",
            "1942\n",
            "1943\n",
            "1944\n",
            "1945\n",
            "1946\n",
            "1947\n",
            "1948\n",
            "1949\n",
            "1950\n",
            "1951\n",
            "1952\n",
            "1953\n",
            "1954\n",
            "1955\n",
            "1956\n",
            "1957\n",
            "1958\n",
            "1959\n",
            "1960\n",
            "1961\n",
            "1962\n",
            "1963\n",
            "1964\n",
            "1965\n",
            "1966\n",
            "1967\n",
            "1968\n",
            "1969\n",
            "1970\n",
            "1971\n",
            "1972\n",
            "1973\n",
            "1974\n",
            "1975\n",
            "1976\n",
            "1977\n",
            "1978\n",
            "1979\n",
            "1980\n",
            "1981\n",
            "1982\n",
            "1983\n",
            "1984\n",
            "1985\n",
            "1986\n",
            "1987\n",
            "1988\n",
            "1989\n",
            "1990\n",
            "1991\n",
            "1992\n",
            "1993\n",
            "1994\n",
            "1995\n",
            "1996\n",
            "1997\n",
            "1998\n",
            "1999\n",
            "2000\n",
            "2000/2709\n",
            "2001\n",
            "2002\n",
            "2003\n",
            "2004\n",
            "2005\n",
            "2006\n",
            "2007\n",
            "2008\n",
            "2009\n",
            "2010\n",
            "2011\n",
            "2012\n",
            "2013\n",
            "2014\n",
            "2015\n",
            "2016\n",
            "2017\n",
            "2018\n",
            "2019\n",
            "2020\n",
            "2021\n",
            "2022\n",
            "2023\n",
            "2024\n",
            "2025\n",
            "2026\n",
            "2027\n",
            "2028\n",
            "2029\n",
            "2030\n",
            "2031\n",
            "2032\n",
            "2033\n",
            "2034\n",
            "2035\n",
            "2036\n",
            "2037\n",
            "2038\n",
            "2039\n",
            "2040\n",
            "2041\n",
            "2042\n",
            "2043\n",
            "2044\n",
            "2045\n",
            "2046\n",
            "2047\n",
            "2048\n",
            "2049\n",
            "2050\n",
            "2051\n",
            "2052\n",
            "2053\n",
            "2054\n",
            "2055\n",
            "2056\n",
            "2057\n",
            "2058\n",
            "2059\n",
            "2060\n",
            "2061\n",
            "2062\n",
            "2063\n",
            "2064\n",
            "2065\n",
            "2066\n",
            "2067\n",
            "2068\n",
            "2069\n",
            "2070\n",
            "2071\n",
            "2072\n",
            "2073\n",
            "2074\n",
            "2075\n",
            "2076\n",
            "2077\n",
            "2078\n",
            "2079\n",
            "2080\n",
            "2081\n",
            "2082\n",
            "2083\n",
            "2084\n",
            "2085\n",
            "2086\n",
            "2087\n",
            "2088\n",
            "2089\n",
            "2090\n",
            "2091\n",
            "2092\n",
            "2093\n",
            "2094\n",
            "2095\n",
            "2096\n",
            "2097\n",
            "2098\n",
            "2099\n",
            "2100\n",
            "2100/2709\n",
            "2101\n",
            "2102\n",
            "2103\n",
            "2104\n",
            "2105\n",
            "2106\n",
            "2107\n",
            "2108\n",
            "2109\n",
            "2110\n",
            "2111\n",
            "2112\n",
            "2113\n",
            "2114\n",
            "2115\n",
            "2116\n",
            "2117\n",
            "2118\n",
            "2119\n",
            "2120\n",
            "2121\n",
            "2122\n",
            "2123\n",
            "2124\n",
            "2125\n",
            "2126\n",
            "2127\n",
            "2128\n",
            "2129\n",
            "2130\n",
            "2131\n",
            "2132\n",
            "2133\n",
            "2134\n",
            "2135\n",
            "2136\n",
            "2137\n",
            "2138\n",
            "2139\n",
            "2140\n",
            "2141\n",
            "2142\n",
            "2143\n",
            "2144\n",
            "2145\n",
            "2146\n",
            "2147\n",
            "2148\n",
            "2149\n",
            "2150\n",
            "2151\n",
            "2152\n",
            "2153\n",
            "2154\n",
            "2155\n",
            "2156\n",
            "2157\n",
            "2158\n",
            "2159\n",
            "2160\n",
            "2161\n",
            "2162\n",
            "2163\n",
            "2164\n",
            "2165\n",
            "2166\n",
            "2167\n",
            "2168\n",
            "2169\n",
            "2170\n",
            "2171\n",
            "2172\n",
            "2173\n",
            "2174\n",
            "2175\n",
            "2176\n",
            "2177\n",
            "2178\n",
            "2179\n",
            "2180\n",
            "2181\n",
            "2182\n",
            "2183\n",
            "2184\n",
            "2185\n",
            "2186\n",
            "2187\n",
            "2188\n",
            "2189\n",
            "2190\n",
            "2191\n",
            "2192\n",
            "2193\n",
            "2194\n",
            "2195\n",
            "2196\n",
            "2197\n",
            "2198\n",
            "2199\n",
            "2200\n",
            "2200/2709\n",
            "2201\n",
            "2202\n",
            "2203\n",
            "2204\n",
            "2205\n",
            "2206\n",
            "2207\n",
            "2208\n",
            "2209\n",
            "2210\n",
            "2211\n",
            "2212\n",
            "2213\n",
            "2214\n",
            "2215\n",
            "2216\n",
            "2217\n",
            "2218\n",
            "2219\n",
            "2220\n",
            "2221\n",
            "2222\n",
            "2223\n",
            "2224\n",
            "2225\n",
            "2226\n",
            "2227\n",
            "2228\n",
            "2229\n",
            "2230\n",
            "2231\n",
            "2232\n",
            "2233\n",
            "2234\n",
            "2235\n",
            "2236\n",
            "2237\n",
            "2238\n",
            "2239\n",
            "2240\n",
            "2241\n",
            "2242\n",
            "2243\n",
            "2244\n",
            "2245\n",
            "2246\n",
            "2247\n",
            "2248\n",
            "2249\n",
            "2250\n",
            "2251\n",
            "2252\n",
            "2253\n",
            "2254\n",
            "2255\n",
            "2256\n",
            "2257\n",
            "2258\n",
            "2259\n",
            "2260\n",
            "2261\n",
            "2262\n",
            "2263\n",
            "2264\n",
            "2265\n",
            "2266\n",
            "2267\n",
            "2268\n",
            "2269\n",
            "2270\n",
            "2271\n",
            "2272\n",
            "2273\n",
            "2274\n",
            "2275\n",
            "2276\n",
            "2277\n",
            "2278\n",
            "2279\n",
            "2280\n",
            "2281\n",
            "2282\n",
            "2283\n",
            "2284\n",
            "2285\n",
            "2286\n",
            "2287\n",
            "2288\n",
            "2289\n",
            "2290\n",
            "2291\n",
            "2292\n",
            "2293\n",
            "2294\n",
            "2295\n",
            "2296\n",
            "2297\n",
            "2298\n",
            "2299\n",
            "2300\n",
            "2300/2709\n",
            "2301\n",
            "2302\n",
            "2303\n",
            "2304\n",
            "2305\n",
            "2306\n",
            "2307\n",
            "2308\n",
            "2309\n",
            "2310\n",
            "2311\n",
            "2312\n",
            "2313\n",
            "2314\n",
            "2315\n",
            "2316\n",
            "2317\n",
            "2318\n",
            "2319\n",
            "2320\n",
            "2321\n",
            "2322\n",
            "2323\n",
            "2324\n",
            "2325\n",
            "2326\n",
            "2327\n",
            "2328\n",
            "2329\n",
            "2330\n",
            "2331\n",
            "2332\n",
            "2333\n",
            "2334\n",
            "2335\n",
            "2336\n",
            "2337\n",
            "2338\n",
            "2339\n",
            "2340\n",
            "2341\n",
            "2342\n",
            "2343\n",
            "2344\n",
            "2345\n",
            "2346\n",
            "2347\n",
            "2348\n",
            "2349\n",
            "2350\n",
            "2351\n",
            "2352\n",
            "2353\n",
            "2354\n",
            "2355\n",
            "2356\n",
            "2357\n",
            "2358\n",
            "2359\n",
            "2360\n",
            "2361\n",
            "2362\n",
            "2363\n",
            "2364\n",
            "2365\n",
            "2366\n",
            "2367\n",
            "2368\n",
            "2369\n",
            "2370\n",
            "2371\n",
            "2372\n",
            "2373\n",
            "2374\n",
            "2375\n",
            "2376\n",
            "2377\n",
            "2378\n",
            "2379\n",
            "2380\n",
            "2381\n",
            "2382\n",
            "2383\n",
            "2384\n",
            "2385\n",
            "2386\n",
            "2387\n",
            "2388\n",
            "2389\n",
            "2390\n",
            "2391\n",
            "2392\n",
            "2393\n",
            "2394\n",
            "2395\n",
            "2396\n",
            "2397\n",
            "2398\n",
            "2399\n",
            "2400\n",
            "2400/2709\n",
            "2401\n",
            "2402\n",
            "2403\n",
            "2404\n",
            "2405\n",
            "2406\n",
            "2407\n",
            "2408\n",
            "2409\n",
            "2410\n",
            "2411\n",
            "2412\n",
            "2413\n",
            "2414\n",
            "2415\n",
            "2416\n",
            "2417\n",
            "2418\n",
            "2419\n",
            "2420\n",
            "2421\n",
            "2422\n",
            "2423\n",
            "2424\n",
            "2425\n",
            "2426\n",
            "2427\n",
            "2428\n",
            "2429\n",
            "2430\n",
            "2431\n",
            "2432\n",
            "2433\n",
            "2434\n",
            "2435\n",
            "2436\n",
            "2437\n",
            "2438\n",
            "2439\n",
            "2440\n",
            "2441\n",
            "2442\n",
            "2443\n",
            "2444\n",
            "2445\n",
            "2446\n",
            "2447\n",
            "2448\n",
            "2449\n",
            "2450\n",
            "2451\n",
            "2452\n",
            "2453\n",
            "2454\n",
            "2455\n",
            "2456\n",
            "2457\n",
            "2458\n",
            "2459\n",
            "2460\n",
            "2461\n",
            "2462\n",
            "2463\n",
            "2464\n",
            "2465\n",
            "2466\n",
            "2467\n",
            "2468\n",
            "2469\n",
            "2470\n",
            "2471\n",
            "2472\n",
            "2473\n",
            "2474\n",
            "2475\n",
            "2476\n",
            "2477\n",
            "2478\n",
            "2479\n",
            "2480\n",
            "2481\n",
            "2482\n",
            "2483\n",
            "2484\n",
            "2485\n",
            "2486\n",
            "2487\n",
            "2488\n",
            "2489\n",
            "2490\n",
            "2491\n",
            "2492\n",
            "2493\n",
            "2494\n",
            "2495\n",
            "2496\n",
            "2497\n",
            "2498\n",
            "2499\n",
            "2500\n",
            "2500/2709\n",
            "2501\n",
            "2502\n",
            "2503\n",
            "2504\n",
            "2505\n",
            "2506\n",
            "2507\n",
            "2508\n",
            "2509\n",
            "2510\n",
            "2511\n",
            "2512\n",
            "2513\n",
            "2514\n",
            "2515\n",
            "2516\n",
            "2517\n",
            "2518\n",
            "2519\n",
            "2520\n",
            "2521\n",
            "2522\n",
            "2523\n",
            "2524\n",
            "2525\n",
            "2526\n",
            "2527\n",
            "2528\n",
            "2529\n",
            "2530\n",
            "2531\n",
            "2532\n",
            "2533\n",
            "2534\n",
            "2535\n",
            "2536\n",
            "2537\n",
            "2538\n",
            "2539\n",
            "2540\n",
            "2541\n",
            "2542\n",
            "2543\n",
            "2544\n",
            "2545\n",
            "2546\n",
            "2547\n",
            "2548\n",
            "2549\n",
            "2550\n",
            "2551\n",
            "2552\n",
            "2553\n",
            "2554\n",
            "2555\n",
            "2556\n",
            "2557\n",
            "2558\n",
            "2559\n",
            "2560\n",
            "2561\n",
            "2562\n",
            "2563\n",
            "2564\n",
            "2565\n",
            "2566\n",
            "2567\n",
            "2568\n",
            "2569\n",
            "2570\n",
            "2571\n",
            "2572\n",
            "2573\n",
            "2574\n",
            "2575\n",
            "2576\n",
            "2577\n",
            "2578\n",
            "2579\n",
            "2580\n",
            "2581\n",
            "2582\n",
            "2583\n",
            "2584\n",
            "2585\n",
            "2586\n",
            "2587\n",
            "2588\n",
            "2589\n",
            "2590\n",
            "2591\n",
            "2592\n",
            "2593\n",
            "2594\n",
            "2595\n",
            "2596\n",
            "2597\n",
            "2598\n",
            "2599\n",
            "2600\n",
            "2600/2709\n",
            "2601\n",
            "2602\n",
            "2603\n",
            "2604\n",
            "2605\n",
            "2606\n",
            "2607\n",
            "2608\n",
            "2609\n",
            "2610\n",
            "2611\n",
            "2612\n",
            "2613\n",
            "2614\n",
            "2615\n",
            "2616\n",
            "2617\n",
            "2618\n",
            "2619\n",
            "2620\n",
            "2621\n",
            "2622\n",
            "2623\n",
            "2624\n",
            "2625\n",
            "2626\n",
            "2627\n",
            "2628\n",
            "2629\n",
            "2630\n",
            "2631\n",
            "2632\n",
            "2633\n",
            "2634\n",
            "2635\n",
            "2636\n",
            "2637\n",
            "2638\n",
            "2639\n",
            "2640\n",
            "2641\n",
            "2642\n",
            "2643\n",
            "2644\n",
            "2645\n",
            "2646\n",
            "2647\n",
            "2648\n",
            "2649\n",
            "2650\n",
            "2651\n",
            "2652\n",
            "2653\n",
            "2654\n",
            "2655\n",
            "2656\n",
            "2657\n",
            "2658\n",
            "2659\n",
            "2660\n",
            "2661\n",
            "2662\n",
            "2663\n",
            "2664\n",
            "2665\n",
            "2666\n",
            "2667\n",
            "2668\n",
            "2669\n",
            "2670\n",
            "2671\n",
            "2672\n",
            "2673\n",
            "2674\n",
            "2675\n",
            "2676\n",
            "2677\n",
            "2678\n",
            "2679\n",
            "2680\n",
            "2681\n",
            "2682\n",
            "2683\n",
            "2684\n",
            "2685\n",
            "2686\n",
            "2687\n",
            "2688\n",
            "2689\n",
            "2690\n",
            "2691\n",
            "2692\n",
            "2693\n",
            "2694\n",
            "2695\n",
            "2696\n",
            "2697\n",
            "2698\n",
            "2699\n",
            "2700\n",
            "2700/2709\n",
            "2701\n",
            "2702\n",
            "2703\n",
            "2704\n",
            "2705\n",
            "2706\n",
            "2707\n",
            "2708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsRntQeZNl85",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fe506489-7b9e-40e8-f16a-08ffe22690a4"
      },
      "source": [
        "len(all_results) #Usar após o previsor contabilizar todos os resultados da fórmula anterior."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10836"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xrvJQyYQfx-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "76b8d8c2-c40c-4bb9-add0-e480bd2d4181"
      },
      "source": [
        "all_results[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RawResult(unique_id=1000000000, start_logits=[-5.680498123168945, -6.508359909057617, -6.347847938537598, -6.409255027770996, -6.601614475250244, -6.679850101470947, -6.885415554046631, -6.44765043258667, -5.096132278442383, -6.475470066070557, -6.751291275024414, -6.835025310516357, -6.549988746643066, -2.6234476566314697, -5.800776481628418, -5.870785713195801, -6.058463096618652, -5.638486385345459, -4.699994087219238, -5.976578712463379, -6.298312187194824, -5.913740158081055, -5.703412055969238, -5.316441059112549, -4.532556533813477, -6.576051712036133, -5.578375816345215, -3.922252893447876, -6.0888142585754395, -6.634547233581543, -6.550261974334717, -5.494431018829346, -6.453731536865234, -6.363158226013184, -5.866878032684326, -5.3049139976501465, -6.713850975036621, -6.036337852478027, -1.3589874505996704, 2.1116151809692383, -5.711420059204102, -6.098797798156738, -6.323404788970947, -3.880269765853882, -4.868359565734863, -3.1288504600524902, 4.335103988647461, -1.7793662548065186, -4.628854751586914, -2.9301764965057373, -2.0632128715515137, -6.238362789154053, -6.794212341308594, -6.45980167388916, -3.978240728378296, -6.6034345626831055, -5.089744567871094, 2.384946346282959, -2.885601043701172, -4.948498725891113, -5.6900858879089355, -6.3077802658081055, -5.873169898986816, -5.485106468200684, -5.067986488342285, -5.464627265930176, -5.163266181945801, -6.587039947509766, -6.795533180236816, -6.00816535949707, -5.752101898193359, -5.883691787719727, -6.057443618774414, -6.082065582275391, -5.63547420501709, -5.009467124938965, -6.422192573547363, -6.698077201843262, -6.042151927947998, -6.769956588745117, -5.627224922180176, -4.639443397521973, -6.541045188903809, -6.612759590148926, -6.515566825866699, -6.528090953826904, -6.126003265380859, -5.444439888000488, -6.422535419464111, -6.620682716369629, -7.146787643432617, -6.47784423828125, -5.341459274291992, -6.737522602081299, -6.8785576820373535, -6.531682014465332, -6.812182426452637, -6.047836780548096, -6.1483564376831055, -6.260523796081543, -5.902325630187988, -5.811334609985352, -6.050166606903076, -6.957348823547363, -7.0340423583984375, -5.871948719024658, -6.281970024108887, -6.1403069496154785, -6.116413116455078, -6.201160430908203, -5.381449222564697, -7.144896507263184, -7.251642227172852, -6.877045631408691, -6.341297626495361, -6.251185417175293, -6.982161521911621, -7.059152126312256, -7.209488868713379, -7.223772048950195, -6.795295715332031, -6.963513374328613, -6.847575664520264, -6.301562309265137, -6.449856758117676, -6.877419471740723, -6.575865745544434, -6.651981830596924, -6.92788553237915, -6.347855091094971, -6.4002532958984375, -6.093225479125977, -6.73525333404541, -7.209096908569336, -6.602264404296875, -6.008152961730957, -6.856050491333008, -7.15056037902832, -7.256003379821777, -7.031932353973389, -6.794162273406982, -7.184356689453125, -6.553128719329834, -6.667330265045166, -6.496904373168945, -6.458409786224365, -6.439627170562744, -6.542086601257324, -6.510972023010254, -6.107837677001953, -4.928524494171143, -6.636085033416748, -6.855815887451172, -7.0572662353515625, -7.305466651916504, -7.058799743652344, -6.305838108062744, -5.965856075286865, -5.754232406616211, -5.885453224182129, -6.0288519859313965, -6.289608001708984, -6.3482346534729, -5.4890570640563965, -4.343374252319336, -6.356451034545898, -6.589565277099609, -6.567920684814453, -6.698624610900879, -6.16296911239624, -6.539628028869629, -6.912808895111084, -6.924792766571045, -6.917418479919434, -6.918011665344238, -6.90844202041626, -6.910329818725586, -6.913090229034424, -6.92193603515625, -6.910037994384766, -6.920892238616943, -6.913631439208984, -6.906924247741699, -6.899117469787598, -6.899065017700195, -6.9142608642578125, -6.902215957641602, -6.897931098937988, -6.8940629959106445, -6.885372161865234, -6.893834114074707, -6.895448684692383, -6.897065162658691, -6.902773857116699, -6.899072647094727, -6.899473190307617, -6.901543617248535, -6.907754421234131, -6.907116889953613, -6.9253950119018555, -6.940725326538086, -6.921504974365234, -6.932624340057373, -6.941987037658691, -6.916254997253418, -6.930120468139648, -6.92426872253418, -6.923039436340332, -6.910819053649902, -6.906466007232666, -6.914017677307129, -6.935773849487305, -6.913149356842041, -6.926677227020264, -6.919105052947998, -6.916637420654297, -6.934001922607422, -6.918948173522949, -6.908360004425049, -6.904017448425293, -6.905278205871582, -6.908542633056641, -6.899229526519775, -6.900178909301758, -6.900601387023926, -6.907915115356445, -6.9049201011657715, -6.900053024291992, -6.913460731506348, -6.922430038452148, -6.923526763916016, -6.922092914581299, -6.928667068481445, -6.922214031219482, -6.9129838943481445, -6.920100212097168, -6.927005767822266, -6.919814109802246, -6.974222183227539, -6.932468891143799, -6.9134297370910645, -6.936396598815918, -6.981756687164307, -6.935485363006592, -7.006120681762695, -6.947304725646973, -6.936838150024414, -6.999831199645996, -6.992127895355225, -6.961690902709961, -7.087702751159668, -6.951292037963867, -6.947778224945068, -6.97989559173584, -6.947054386138916, -6.95945930480957, -6.967470169067383, -6.982912063598633, -6.930983543395996, -6.9793195724487305, -6.955577850341797, -6.9413042068481445, -6.957846164703369, -6.944952964782715, -6.926466464996338, -6.92536735534668, -6.923102378845215, -6.909747123718262, -6.919769763946533, -6.90660285949707, -6.9041547775268555, -6.904670715332031, -6.901394367218018, -6.908812522888184, -6.902557849884033, -6.917363166809082, -6.922749042510986, -6.92748498916626, -6.926464080810547, -6.928504467010498, -6.920715808868408, -6.927872657775879, -6.946804046630859, -6.93730354309082, -6.917404651641846, -6.9207658767700195, -6.934268951416016, -6.923065185546875, -6.918756484985352, -6.9185590744018555, -6.907749176025391, -6.927974700927734, -6.934366226196289, -6.914154052734375, -6.9288177490234375, -6.939456939697266, -6.930187225341797, -6.941170692443848, -6.928736686706543, -6.92228889465332, -6.92417049407959, -6.917040824890137, -6.92317533493042, -6.914550304412842, -6.925277233123779, -6.920083045959473, -6.915041923522949, -6.917016983032227, -6.913734436035156, -6.909085273742676, -6.914045333862305, -6.913120269775391, -6.919449329376221, -6.940976142883301, -6.9278764724731445, -6.9407196044921875, -6.938072204589844, -6.920350074768066, -6.928803443908691, -6.925980567932129, -6.918434143066406, -6.921131134033203, -6.92366361618042, -6.917421817779541, -6.920686721801758, -6.911216735839844, -6.902151584625244, -6.9051103591918945, -6.905806064605713, -6.903385639190674, -6.906432151794434, -6.9051713943481445, -6.906454563140869, -6.915868759155273, -6.921365737915039, -6.927404403686523, -6.922240257263184, -6.919198513031006, -6.922308921813965, -6.926853179931641, -6.944181442260742, -6.9212470054626465, -6.9130449295043945, -6.921657562255859, -6.9271955490112305, -6.923557281494141, -6.913084983825684, -6.911466598510742, -6.916985511779785, -6.931839466094971, -6.923746585845947, -6.9158935546875, -6.915136814117432, -6.907498359680176, -6.909745216369629, -6.913168907165527, -6.90490198135376, -6.907689094543457, -6.910182476043701, -6.9096503257751465, -6.919586181640625, -6.97268533706665, -6.963364601135254, -6.938351631164551, -6.983864784240723, -7.035139560699463, -6.9479875564575195, -7.002886772155762, -6.958965301513672, -6.9224138259887695, -6.934661388397217, -6.936278343200684, -6.963720321655273, -6.931074619293213, -6.935764312744141, -6.948989391326904, -6.935581207275391, -6.966231822967529, -6.951664447784424, -6.9232635498046875, -6.930830955505371, -6.916734218597412, -6.916229724884033, -6.931075096130371], end_logits=[-5.494173526763916, -5.971158504486084, -5.725264072418213, -5.305186748504639, -6.594335079193115, -6.601078033447266, -5.566329479217529, -6.506786346435547, -6.963768482208252, -6.371024131774902, -4.975831508636475, -6.774938583374023, -6.404515743255615, -6.325263500213623, -5.662225246429443, -4.0074896812438965, -6.153436660766602, -6.492265224456787, -6.150218963623047, -4.3065104484558105, -4.880399227142334, -6.672396183013916, -6.7403154373168945, -6.5658040046691895, -5.641432285308838, -6.147932529449463, -6.349369525909424, -6.119655132293701, -5.465841770172119, -4.701919078826904, -5.846664905548096, -3.7297580242156982, -3.144270896911621, -6.443215847015381, -6.651468276977539, -5.897722244262695, -4.848332405090332, -2.736240863800049, -4.841612339019775, -3.7718915939331055, -5.493531703948975, -3.1538102626800537, -5.673675537109375, -4.03386926651001, -2.8426475524902344, -2.0607151985168457, -1.6851409673690796, 4.067271709442139, -4.989521503448486, -4.914733409881592, -5.373447418212891, -5.64169454574585, -5.104461193084717, -5.972296714782715, -3.1895155906677246, -4.851290225982666, -3.489198923110962, -3.0046005249023438, 1.9465854167938232, -5.535470485687256, -4.670391082763672, -3.088473320007324, -6.594631671905518, -6.605160236358643, -6.589206218719482, -5.943670272827148, -6.726982593536377, -5.6854729652404785, -4.380326747894287, -2.668606996536255, -6.736068248748779, -5.991825103759766, -6.663564682006836, -6.382569789886475, -6.4191575050354, -6.548557758331299, -5.94459867477417, -5.630450248718262, -4.623575687408447, -5.542880535125732, -6.327536106109619, -6.5566606521606445, -5.926001071929932, -5.45265007019043, -4.298783779144287, -6.046977519989014, -6.413060188293457, -6.762879371643066, -5.953920841217041, -5.804092884063721, -4.22189474105835, -6.0718255043029785, -6.855742931365967, -5.63505220413208, -5.701992988586426, -2.364569664001465, -5.725935459136963, -6.807285308837891, -6.711907863616943, -6.584902286529541, -6.747134208679199, -6.255642890930176, -6.404576778411865, -4.357916355133057, -5.789036750793457, -6.636148452758789, -5.359255313873291, -6.636000156402588, -6.654325008392334, -6.598459720611572, -6.637251377105713, -4.407806396484375, -4.7129998207092285, -6.133143901824951, -6.477790355682373, -6.274234294891357, -6.149094104766846, -5.571169376373291, -5.117135524749756, -5.491317272186279, -6.275355815887451, -6.172489643096924, -6.2631402015686035, -6.498310565948486, -6.287012100219727, -6.210968494415283, -6.513346195220947, -6.0063042640686035, -6.211963176727295, -6.304098606109619, -6.392937183380127, -6.546024322509766, -5.916670322418213, -5.0813307762146, -6.299027442932129, -6.357326030731201, -5.995224475860596, -5.09030294418335, -4.544173717498779, -5.694735050201416, -6.425683975219727, -5.990582466125488, -6.5235443115234375, -5.980801105499268, -6.525552749633789, -6.363050937652588, -6.66331672668457, -6.495517253875732, -6.581193923950195, -6.6826558113098145, -6.84745979309082, -6.011455535888672, -4.844701290130615, -4.126136302947998, -4.102977752685547, -5.305083751678467, -6.507308006286621, -6.825521469116211, -6.7150654792785645, -5.769296646118164, -6.543702602386475, -6.440351963043213, -5.9236063957214355, -6.347151279449463, -5.7667107582092285, -5.904757976531982, -5.061546802520752, -5.147802829742432, -3.43379545211792, -2.9325952529907227, -6.403752326965332, -6.896340847015381, -6.88929557800293, -6.897389888763428, -6.899444580078125, -6.904217720031738, -6.9018473625183105, -6.895822048187256, -6.876708507537842, -6.8800177574157715, -6.872007369995117, -6.879711627960205, -6.883349895477295, -6.882512092590332, -6.882100582122803, -6.871403217315674, -6.8932623863220215, -6.901395797729492, -6.904239177703857, -6.913994312286377, -6.908114910125732, -6.909404277801514, -6.9082560539245605, -6.907773494720459, -6.910606861114502, -6.913254261016846, -6.916125774383545, -6.904292583465576, -6.908279895782471, -6.890717029571533, -6.885733604431152, -6.891078472137451, -6.8790717124938965, -6.887919902801514, -6.893807888031006, -6.878467082977295, -6.8880839347839355, -6.886066913604736, -6.897736549377441, -6.902117729187012, -6.888023376464844, -6.858681678771973, -6.886791706085205, -6.863561153411865, -6.864284038543701, -6.860654354095459, -6.836830139160156, -6.86096715927124, -6.877440929412842, -6.887588977813721, -6.886791706085205, -6.88746976852417, -6.900749206542969, -6.898501873016357, -6.896761417388916, -6.886552333831787, -6.8939208984375, -6.902619361877441, -6.879847526550293, -6.868416786193848, -6.870936870574951, -6.874300956726074, -6.867653846740723, -6.888645172119141, -6.892649173736572, -6.8880391120910645, -6.896273136138916, -6.896928787231445, -6.850999355316162, -6.889881610870361, -6.902314186096191, -6.8874192237854, -6.845770835876465, -6.8881731033325195, -6.815357685089111, -6.881960391998291, -6.8863043785095215, -6.827395915985107, -6.83702278137207, -6.865964889526367, -6.595849990844727, -6.877282619476318, -6.8715500831604, -6.839837551116943, -6.873405933380127, -6.858314037322998, -6.838769912719727, -6.80061674118042, -6.874551296234131, -6.808709621429443, -6.843663692474365, -6.863640308380127, -6.827037334442139, -6.8461127281188965, -6.876186847686768, -6.879875659942627, -6.889675617218018, -6.903174877166748, -6.890193462371826, -6.905429363250732, -6.907249927520752, -6.909809589385986, -6.9138007164001465, -6.90525484085083, -6.912785053253174, -6.8985819816589355, -6.8975749015808105, -6.898406505584717, -6.889706134796143, -6.888977527618408, -6.899149417877197, -6.895097732543945, -6.884679317474365, -6.890528678894043, -6.902975559234619, -6.900145530700684, -6.888807773590088, -6.901011943817139, -6.9001336097717285, -6.898855209350586, -6.908154010772705, -6.887826442718506, -6.875556468963623, -6.8977837562561035, -6.877707481384277, -6.856323719024658, -6.868196487426758, -6.853672504425049, -6.869361877441406, -6.887284278869629, -6.883591175079346, -6.888482570648193, -6.883172035217285, -6.897883415222168, -6.890280723571777, -6.897785663604736, -6.899366855621338, -6.898271083831787, -6.9020609855651855, -6.907820701599121, -6.906282424926758, -6.9093475341796875, -6.903788089752197, -6.892868518829346, -6.902705669403076, -6.886024475097656, -6.887324810028076, -6.9032979011535645, -6.89305305480957, -6.897292613983154, -6.904322147369385, -6.894008159637451, -6.889268398284912, -6.894508361816406, -6.891546726226807, -6.902172565460205, -6.908845901489258, -6.907857894897461, -6.908998012542725, -6.91156530380249, -6.90922737121582, -6.914852619171143, -6.909505367279053, -6.900915622711182, -6.901811122894287, -6.9026875495910645, -6.903439044952393, -6.90193510055542, -6.901540279388428, -6.895400524139404, -6.886611461639404, -6.903493881225586, -6.9059224128723145, -6.901428699493408, -6.894906520843506, -6.896028518676758, -6.895874977111816, -6.892579555511475, -6.887622356414795, -6.864021301269531, -6.8721923828125, -6.883909225463867, -6.884039402008057, -6.8872456550598145, -6.887348175048828, -6.891445159912109, -6.90637731552124, -6.90574836730957, -6.905724048614502, -6.907994747161865, -6.898321628570557, -6.853058815002441, -6.86242151260376, -6.884688854217529, -6.843996047973633, -6.148624897003174, -6.873161792755127, -6.756247043609619, -6.826239585876465, -6.899181842803955, -6.893825054168701, -6.8904876708984375, -6.8688130378723145, -6.897213459014893, -6.88719367980957, -6.873380184173584, -6.884424209594727, -6.842552661895752, -6.860737323760986, -6.894035816192627, -6.892479419708252, -6.901129722595215, -6.903127193450928, -6.886978626251221])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ySb1nE4QoCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_prediction_file = \"/content/drive/My Drive/Google Colab/BERT/predictions.json\"\n",
        "output_nbest_file = \"/content/drive/My Drive/Google Colab/BERT/nbest_predictions.json\"\n",
        "output_null_log_odds_file = \"/content/drive/My Drive/Google Colab/BERT/null_odds.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymutjp2ORePy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write_predictions(\n",
        "    eval_examples,\n",
        "    eval_features,\n",
        "    all_results,\n",
        "    20,\n",
        "    30,\n",
        "    True,\n",
        "    output_prediction_file,\n",
        "    output_nbest_file,\n",
        "    output_null_log_odds_file,\n",
        "    verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wmLuOBJSZOw",
        "colab_type": "text"
      },
      "source": [
        "Fazer previsões personalizadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBrHUyvcU5cT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> Dicionário de Inputs - Essencial\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBwaEZOESbrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable = False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdJWHihSVHQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_whitespace(c):\n",
        "  if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "    return True\n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdF9jQuXVhBX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a9c153f3-a075-43a5-e087-a245dbc16492"
      },
      "source": [
        "is_whitespace(\"a\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N2AoljJVkBg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "44eb72b7-4171-462a-b91f-798f57d31202"
      },
      "source": [
        "is_whitespace(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCEbfrXeVnCa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d2c70149-5dc4-49da-dca8-fde6dfb60bdb"
      },
      "source": [
        "is_whitespace(\" \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-oPwHPlVpEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def whitespace_split(text):\n",
        "  doc_tokens = []\n",
        "  prev_is_whitespace = True\n",
        "  for c in text:\n",
        "    if is_whitespace(c):\n",
        "      prev_is_whitespace = True\n",
        "    else:\n",
        "      if prev_is_whitespace:\n",
        "        doc_tokens.append(c)\n",
        "      else:\n",
        "        doc_tokens[-1] += c\n",
        "      prev_is_whitespace = False\n",
        "  return doc_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJnRvaMJWX8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b4b23726-5d09-4b9d-aa13-3db12cdb3028"
      },
      "source": [
        "whitespace_split(\"My dog likes strawberries.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My', 'dog', 'likes', 'strawberries.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWsDlfd3AMy2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "55080f02-f381-493a-e156-285fbb7d89eb"
      },
      "source": [
        "tokenizer.tokenize(\"my\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwT7jmqtAP8m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a9db826c-e411-4510-8aae-1416e7aa12c0"
      },
      "source": [
        "tokenizer.tokenize(\"strawberries\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['straw', '##berries']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmYQZSgHAsFr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cb2d6b6e-1ed7-41e6-e0dc-de33d370884c"
      },
      "source": [
        "t = tokenizer.tokenize(\"strawberries\")\n",
        "t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['straw', '##berries']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdvlQeKfAwBe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9c065d57-c23f-42e0-e51c-0e349ce1486d"
      },
      "source": [
        "len(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MykVZ-zDAzXE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eb60cbc8-9356-4bd1-b571-0a85f5417720"
      },
      "source": [
        "[1] * len(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rk2og0k_d41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_context(text_words):\n",
        "  text_tok = []\n",
        "  tok_to_word_id = []\n",
        "  for word_id, word in enumerate(text_words):\n",
        "    word_tok = tokenizer.tokenize(word)\n",
        "    text_tok += word_tok\n",
        "    tok_to_word_id += [word_id] * len(word_tok)\n",
        "  return text_tok, tok_to_word_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAUJJJQNBGiW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "906efbb1-ce84-4651-a036-c52c46ee32cf"
      },
      "source": [
        "tokenize_context(whitespace_split(\"My dog likes strawberries.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['my', 'dog', 'likes', 'straw', '##berries', '.'], [0, 1, 2, 3, 3, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8NeHazwBmFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ids(tokens):\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "def get_mask(tokens):\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
        "\n",
        "def get_segments(tokens):\n",
        "    seg_ids = []\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":\n",
        "           current_seg_id = 1 - current_seg_id\n",
        "    return seg_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JghbDq6DChrX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_context = '''One of its earliest massive implementations was brought about by Egyptians against the British occupation in the 1919 Revolution. Civil disobedience is one of the many ways people have rebelled against what they deem to be unfair laws. It has been used in many nonviolent resistance movements in India (Gandhi's campaigns for independence from the British Empire), in Czechoslovakia's Velvet Revolution and in East Germany to oust their communist governments, In South Africa in the fight against apartheid, in the American Civil Rights Movement, in the Singing Revolution to bring independence to the Baltic countries from the Soviet Union, recently with the 2003 Rose Revolution in Georgia and the 2004 Orange Revolution in Ukraine, among other various movements worldwide.'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmZ2fEX3EEZB",
        "colab_type": "text"
      },
      "source": [
        "One of its earliest massive implementations was brought about by Egyptians against the British occupation in the 1919 Revolution. Civil disobedience is one of the many ways people have rebelled against what they deem to be unfair laws. It has been used in many nonviolent resistance movements in India (Gandhi's campaigns for independence from the British Empire), in Czechoslovakia's Velvet Revolution and in East Germany to oust their communist governments, In South Africa in the fight against apartheid, in the American Civil Rights Movement, in the Singing Revolution to bring independence to the Baltic countries from the Soviet Union, recently with the 2003 Rose Revolution in Georgia and the 2004 Orange Revolution in Ukraine, among other various movements worldwide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVE0DzJWEGAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_question = '''What is civil disobedience?'''\n",
        "#my_question = '''When the civil disobedience began?'''\n",
        "#my_question = '''What was the the movement called that brought Baltic countries independence from the Soviet Union?'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOOftdZBEvW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_input_dict(question, context):\n",
        "  question_tok = tokenizer.tokenize(my_question)\n",
        "  #print(question_tok)\n",
        "\n",
        "  context_words = whitespace_split(context)\n",
        "  #print(context_words)\n",
        "  context_tok, context_tok_to_word_id = tokenize_context(context_words)\n",
        "  #print(context_tok)\n",
        "  #print(context_tok_to_word_id)\n",
        "\n",
        "  input_tok = question_tok + [\"[SEP]\"] + context_tok + [\"[SEP]\"]\n",
        "  #print(input_tok)\n",
        "  input_tok += [\"[PAD]\"]*(384-len(input_tok))\n",
        "  #print(input_tok)\n",
        "\n",
        "  input_dict = {}\n",
        "  input_dict[\"input_word_ids\"] = tf.expand_dims(tf.cast(get_ids(input_tok), tf.int32), 0)\n",
        "  input_dict[\"input_mask\"] = tf.expand_dims(tf.cast(get_mask(input_tok), tf.int32), 0)\n",
        "  input_dict[\"input_type_ids\"] = tf.expand_dims(tf.cast(get_segments(input_tok), tf.int32), 0)\n",
        "\n",
        "  return input_dict, context_words, context_tok_to_word_id, len(question_tok)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um6DGGtHE4ut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_input_dict, my_context_words, context_tok_to_word_id, question_tok_len = create_input_dict(my_question, my_context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOV0eyTrFN6_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "outputId": "bcf8358a-31a1-4c56-8755-1153df27f709"
      },
      "source": [
        "my_input_dict[\"input_word_ids\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 384), dtype=int32, numpy=\n",
              "array([[ 2054,  2003,  2942,  4487,  6499,  8270, 13684,  1029,   102,\n",
              "         2028,  1997,  2049,  5700,  5294, 24977,  2001,  2716,  2055,\n",
              "         2011, 23437,  2114,  1996,  2329,  6139,  1999,  1996,  4529,\n",
              "         4329,  1012,  2942,  4487,  6499,  8270, 13684,  2003,  2028,\n",
              "         1997,  1996,  2116,  3971,  2111,  2031, 25183,  2114,  2054,\n",
              "         2027,  9266,  2213,  2000,  2022, 15571,  4277,  1012,  2009,\n",
              "         2038,  2042,  2109,  1999,  2116,  2512, 25500, 16136,  5012,\n",
              "         5750,  1999,  2634,  1006, 12338,  1005,  1055,  8008,  2005,\n",
              "         4336,  2013,  1996,  2329,  3400,  1007,  1010,  1999, 12833,\n",
              "         1005,  1055, 10966,  4329,  1998,  1999,  2264,  2762,  2000,\n",
              "        15068,  3367,  2037,  4750,  6867,  1010,  1999,  2148,  3088,\n",
              "         1999,  1996,  2954,  2114, 17862,  1010,  1999,  1996,  2137,\n",
              "         2942,  2916,  2929,  1010,  1999,  1996,  4823,  4329,  2000,\n",
              "         3288,  4336,  2000,  1996, 11275,  3032,  2013,  1996,  3354,\n",
              "         2586,  1010,  3728,  2007,  1996,  2494,  3123,  4329,  1999,\n",
              "         4108,  1998,  1996,  2432,  4589,  4329,  1999,  5924,  1010,\n",
              "         2426,  2060,  2536,  5750,  4969,  1012,   102,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUGZ9qHXHuVg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "3dc0cd69-5875-4773-d805-c95865742dba"
      },
      "source": [
        "my_input_dict[\"input_mask\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 384), dtype=int32, numpy=\n",
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGo1ZN9OH4Jt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "e1b9e5ea-b3b7-4df1-ca29-8123dfeba662"
      },
      "source": [
        "my_input_dict[\"input_type_ids\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 384), dtype=int32, numpy=\n",
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZCJXXq8H9z0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "982148c2-7876-4f9d-c123-9c746c63d3b8"
      },
      "source": [
        "print(my_context_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['One', 'of', 'its', 'earliest', 'massive', 'implementations', 'was', 'brought', 'about', 'by', 'Egyptians', 'against', 'the', 'British', 'occupation', 'in', 'the', '1919', 'Revolution.', 'Civil', 'disobedience', 'is', 'one', 'of', 'the', 'many', 'ways', 'people', 'have', 'rebelled', 'against', 'what', 'they', 'deem', 'to', 'be', 'unfair', 'laws.', 'It', 'has', 'been', 'used', 'in', 'many', 'nonviolent', 'resistance', 'movements', 'in', 'India', \"(Gandhi's\", 'campaigns', 'for', 'independence', 'from', 'the', 'British', 'Empire),', 'in', \"Czechoslovakia's\", 'Velvet', 'Revolution', 'and', 'in', 'East', 'Germany', 'to', 'oust', 'their', 'communist', 'governments,', 'In', 'South', 'Africa', 'in', 'the', 'fight', 'against', 'apartheid,', 'in', 'the', 'American', 'Civil', 'Rights', 'Movement,', 'in', 'the', 'Singing', 'Revolution', 'to', 'bring', 'independence', 'to', 'the', 'Baltic', 'countries', 'from', 'the', 'Soviet', 'Union,', 'recently', 'with', 'the', '2003', 'Rose', 'Revolution', 'in', 'Georgia', 'and', 'the', '2004', 'Orange', 'Revolution', 'in', 'Ukraine,', 'among', 'other', 'various', 'movements', 'worldwide.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUFiSB1BIPBK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ec509a2b-fdd3-4e4e-82bf-e55ea047c7e1"
      },
      "source": [
        "print(context_tok_to_word_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 18, 19, 20, 20, 20, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 33, 34, 35, 36, 37, 37, 38, 39, 40, 41, 42, 43, 44, 44, 44, 45, 46, 47, 48, 49, 49, 49, 49, 50, 51, 52, 53, 54, 55, 56, 56, 56, 57, 58, 58, 58, 59, 60, 61, 62, 63, 64, 65, 66, 66, 67, 68, 69, 69, 70, 71, 72, 73, 74, 75, 76, 77, 77, 78, 79, 80, 81, 82, 83, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 113, 114, 115, 116, 117, 118, 118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlwhQWSuIQ1W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bc983ca8-be2f-4607-8c9b-002a0d89a7b1"
      },
      "source": [
        "question_tok_len"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6K6X2qWPTju",
        "colab_type": "text"
      },
      "source": [
        "Previsões"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mze7vH4OITpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#end_logits, start_logits   = bert_squad(my_input_dict, training = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TORl8JsL4WI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_logits, end_logits = bert_squad(my_input_dict, training = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcCTX49DLaQZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a7de011-551d-4e5e-bd43-a61e76887570"
      },
      "source": [
        "start_logits[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(384,), dtype=float32, numpy=\n",
              "array([-4.695386 , -5.34116  , -4.3556075, -6.304845 , -6.235936 ,\n",
              "       -6.5309753, -6.9285088, -6.1335783, -6.4121313, -5.566921 ,\n",
              "       -6.3553395, -5.4073553, -5.5902686, -5.92128  , -6.1695895,\n",
              "       -6.2080164, -5.87314  , -6.3338823, -5.7698708, -5.108214 ,\n",
              "       -5.8242555, -5.7879515, -5.3539906, -6.5561504, -6.011974 ,\n",
              "       -5.33072  , -5.3127875, -6.6688704, -5.7903996, -3.595606 ,\n",
              "       -5.1396422, -5.117301 , -5.3041697, -5.664213 , -2.7623408,\n",
              "       -0.6368595, -4.210598 , -2.6133614, -4.037932 , -3.284277 ,\n",
              "       -3.7254372, -5.0051594, -2.9122784, -4.7608585, -4.117655 ,\n",
              "       -5.2258253, -5.353958 , -6.1201267, -5.4820824, -5.1737804,\n",
              "       -4.5405073, -4.9879394, -5.870536 , -4.427004 , -5.983035 ,\n",
              "       -6.13952  , -5.9632416, -5.6378336, -5.11422  , -4.96364  ,\n",
              "       -6.3711033, -6.641406 , -5.9436207, -6.5833116, -6.132853 ,\n",
              "       -5.629445 , -6.290721 , -5.2178154, -6.5651474, -6.6541653,\n",
              "       -6.323512 , -6.42209  , -5.9281716, -6.889344 , -6.586534 ,\n",
              "       -6.3654556, -7.3317366, -7.222248 , -6.54464  , -5.240746 ,\n",
              "       -5.236367 , -6.2708178, -6.382648 , -5.651717 , -6.8048916,\n",
              "       -6.831742 , -6.159822 , -5.7924857, -6.9386387, -5.8002768,\n",
              "       -5.949168 , -6.4628906, -6.299995 , -6.2350388, -7.160142 ,\n",
              "       -6.7422886, -5.694396 , -5.529559 , -6.811088 , -5.9058113,\n",
              "       -6.11633  , -5.90685  , -6.508749 , -6.751876 , -6.634945 ,\n",
              "       -5.346041 , -5.669156 , -5.299327 , -6.0191526, -6.8337593,\n",
              "       -6.8477454, -6.5303063, -5.28586  , -5.6834593, -5.454891 ,\n",
              "       -7.0777845, -6.0928254, -6.1578827, -6.5856133, -6.826722 ,\n",
              "       -6.6186132, -6.594904 , -7.3049216, -6.7554865, -6.4973106,\n",
              "       -6.378972 , -7.493492 , -6.825755 , -6.5999565, -6.6013594,\n",
              "       -5.740076 , -5.6138372, -6.1130185, -7.074584 , -6.8609858,\n",
              "       -6.8140106, -6.8634825, -6.4675846, -6.4022903, -6.603592 ,\n",
              "       -7.1761885, -6.961747 , -7.0557446, -6.986332 , -6.7070723,\n",
              "       -6.526787 , -6.5549726, -6.8487325, -7.290841 , -6.10424  ,\n",
              "       -6.4031777, -6.8071413, -6.7911777, -6.7908006, -6.821908 ,\n",
              "       -6.7966075, -6.803641 , -6.816306 , -6.838343 , -6.840722 ,\n",
              "       -6.8379765, -6.8160834, -6.810079 , -6.7904434, -6.823912 ,\n",
              "       -6.8311577, -6.822543 , -6.83452  , -6.857885 , -6.8542585,\n",
              "       -6.8616495, -6.87052  , -6.864865 , -6.874785 , -6.8676195,\n",
              "       -6.8495703, -6.8274355, -6.820116 , -6.7967634, -6.785268 ,\n",
              "       -6.699244 , -6.7778254, -6.804502 , -6.8120503, -6.780686 ,\n",
              "       -6.815651 , -6.808325 , -6.815999 , -6.8416767, -6.8249683,\n",
              "       -6.8145328, -6.820552 , -6.8408866, -6.84103  , -6.8512316,\n",
              "       -6.834215 , -6.8164673, -6.8260326, -6.8248615, -6.840006 ,\n",
              "       -6.835105 , -6.8175564, -6.8379126, -6.825588 , -6.820324 ,\n",
              "       -6.844896 , -6.842037 , -6.846923 , -6.862441 , -6.8430862,\n",
              "       -6.8319464, -6.8376465, -6.8223753, -6.8043427, -6.855044 ,\n",
              "       -6.7818027, -6.8001404, -6.809898 , -6.7937336, -6.8209715,\n",
              "       -6.799378 , -6.828141 , -6.8359413, -6.8379807, -6.8124194,\n",
              "       -6.8204846, -6.83514  , -6.8771687, -6.8115625, -6.80627  ,\n",
              "       -6.8042088, -6.810461 , -6.81897  , -6.7797728, -6.8172274,\n",
              "       -6.831664 , -6.837389 , -6.846701 , -6.8555145, -6.837566 ,\n",
              "       -6.86582  , -6.8559637, -6.847285 , -6.8567734, -6.814789 ,\n",
              "       -6.844452 , -6.8607316, -6.850774 , -6.8668365, -6.893804 ,\n",
              "       -6.874565 , -6.866338 , -6.893798 , -6.8802567, -6.860512 ,\n",
              "       -6.881175 , -6.885625 , -6.8376255, -6.839115 , -6.8622684,\n",
              "       -6.8694916, -6.8136597, -6.8456182, -6.88194  , -6.8547397,\n",
              "       -6.8688803, -6.8843174, -6.8765125, -6.883712 , -6.8481283,\n",
              "       -6.850874 , -6.855592 , -6.8333426, -6.821801 , -6.792569 ,\n",
              "       -6.811334 , -6.79803  , -6.7925596, -6.7948647, -6.791742 ,\n",
              "       -6.8118515, -6.833867 , -6.844953 , -6.8562956, -6.850803 ,\n",
              "       -6.868211 , -6.856111 , -6.852242 , -6.858617 , -6.8494134,\n",
              "       -6.8216286, -6.839472 , -6.8094225, -6.785757 , -6.8100915,\n",
              "       -6.7888026, -6.771131 , -6.797405 , -6.798974 , -6.8020763,\n",
              "       -6.8167615, -6.794673 , -6.8110476, -6.815769 , -6.833273 ,\n",
              "       -6.8448954, -6.833053 , -6.847465 , -6.84581  , -6.8489285,\n",
              "       -6.841622 , -6.8243885, -6.8212056, -6.820993 , -6.8099384,\n",
              "       -6.8180056, -6.821505 , -6.808032 , -6.8410525, -6.846192 ,\n",
              "       -6.8499646, -6.8541822, -6.85507  , -6.8573904, -6.8639326,\n",
              "       -6.8661876, -6.8581963, -6.856992 , -6.845518 , -6.8378086,\n",
              "       -6.830401 , -6.8422127, -6.7997603, -6.81553  , -6.8116827,\n",
              "       -6.7981386, -6.8185234, -6.827529 , -6.826756 , -6.859711 ,\n",
              "       -6.8566494, -6.8474035, -6.8518944, -6.853066 , -6.854267 ,\n",
              "       -6.8525534, -6.9059057, -6.8540707, -6.8183255, -6.852317 ,\n",
              "       -6.824561 , -6.834565 , -6.871435 , -6.8379583, -6.810804 ,\n",
              "       -6.8238964, -6.8105097, -6.808707 , -6.7920547, -6.7857146,\n",
              "       -6.815107 , -6.7963343, -6.814032 , -6.8304987, -6.8178844,\n",
              "       -6.8339005, -6.848664 , -6.8365707, -6.8513   , -6.869733 ,\n",
              "       -6.8517084, -6.8509264, -6.845194 , -6.819871 , -6.815998 ,\n",
              "       -6.8070593, -6.8025618, -6.8198743, -6.8146806, -6.821604 ,\n",
              "       -6.8328414, -6.833531 , -6.817486 , -6.826071 ], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dgqPTq_US7C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f154216f-cbd5-46b4-fb01-e41769a88bdb"
      },
      "source": [
        "question_tok_len"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7OVvP8KMN1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_logits_context = start_logits.numpy()[0, question_tok_len + 1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWi68XrIMeAf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8eb28f1d-545f-4651-d1e3-31e97fbdf446"
      },
      "source": [
        "start_logits_context"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-5.566921 , -6.3553395, -5.4073553, -5.5902686, -5.92128  ,\n",
              "       -6.1695895, -6.2080164, -5.87314  , -6.3338823, -5.7698708,\n",
              "       -5.108214 , -5.8242555, -5.7879515, -5.3539906, -6.5561504,\n",
              "       -6.011974 , -5.33072  , -5.3127875, -6.6688704, -5.7903996,\n",
              "       -3.595606 , -5.1396422, -5.117301 , -5.3041697, -5.664213 ,\n",
              "       -2.7623408, -0.6368595, -4.210598 , -2.6133614, -4.037932 ,\n",
              "       -3.284277 , -3.7254372, -5.0051594, -2.9122784, -4.7608585,\n",
              "       -4.117655 , -5.2258253, -5.353958 , -6.1201267, -5.4820824,\n",
              "       -5.1737804, -4.5405073, -4.9879394, -5.870536 , -4.427004 ,\n",
              "       -5.983035 , -6.13952  , -5.9632416, -5.6378336, -5.11422  ,\n",
              "       -4.96364  , -6.3711033, -6.641406 , -5.9436207, -6.5833116,\n",
              "       -6.132853 , -5.629445 , -6.290721 , -5.2178154, -6.5651474,\n",
              "       -6.6541653, -6.323512 , -6.42209  , -5.9281716, -6.889344 ,\n",
              "       -6.586534 , -6.3654556, -7.3317366, -7.222248 , -6.54464  ,\n",
              "       -5.240746 , -5.236367 , -6.2708178, -6.382648 , -5.651717 ,\n",
              "       -6.8048916, -6.831742 , -6.159822 , -5.7924857, -6.9386387,\n",
              "       -5.8002768, -5.949168 , -6.4628906, -6.299995 , -6.2350388,\n",
              "       -7.160142 , -6.7422886, -5.694396 , -5.529559 , -6.811088 ,\n",
              "       -5.9058113, -6.11633  , -5.90685  , -6.508749 , -6.751876 ,\n",
              "       -6.634945 , -5.346041 , -5.669156 , -5.299327 , -6.0191526,\n",
              "       -6.8337593, -6.8477454, -6.5303063, -5.28586  , -5.6834593,\n",
              "       -5.454891 , -7.0777845, -6.0928254, -6.1578827, -6.5856133,\n",
              "       -6.826722 , -6.6186132, -6.594904 , -7.3049216, -6.7554865,\n",
              "       -6.4973106, -6.378972 , -7.493492 , -6.825755 , -6.5999565,\n",
              "       -6.6013594, -5.740076 , -5.6138372, -6.1130185, -7.074584 ,\n",
              "       -6.8609858, -6.8140106, -6.8634825, -6.4675846, -6.4022903,\n",
              "       -6.603592 , -7.1761885, -6.961747 , -7.0557446, -6.986332 ,\n",
              "       -6.7070723, -6.526787 , -6.5549726, -6.8487325, -7.290841 ,\n",
              "       -6.10424  , -6.4031777, -6.8071413, -6.7911777, -6.7908006,\n",
              "       -6.821908 , -6.7966075, -6.803641 , -6.816306 , -6.838343 ,\n",
              "       -6.840722 , -6.8379765, -6.8160834, -6.810079 , -6.7904434,\n",
              "       -6.823912 , -6.8311577, -6.822543 , -6.83452  , -6.857885 ,\n",
              "       -6.8542585, -6.8616495, -6.87052  , -6.864865 , -6.874785 ,\n",
              "       -6.8676195, -6.8495703, -6.8274355, -6.820116 , -6.7967634,\n",
              "       -6.785268 , -6.699244 , -6.7778254, -6.804502 , -6.8120503,\n",
              "       -6.780686 , -6.815651 , -6.808325 , -6.815999 , -6.8416767,\n",
              "       -6.8249683, -6.8145328, -6.820552 , -6.8408866, -6.84103  ,\n",
              "       -6.8512316, -6.834215 , -6.8164673, -6.8260326, -6.8248615,\n",
              "       -6.840006 , -6.835105 , -6.8175564, -6.8379126, -6.825588 ,\n",
              "       -6.820324 , -6.844896 , -6.842037 , -6.846923 , -6.862441 ,\n",
              "       -6.8430862, -6.8319464, -6.8376465, -6.8223753, -6.8043427,\n",
              "       -6.855044 , -6.7818027, -6.8001404, -6.809898 , -6.7937336,\n",
              "       -6.8209715, -6.799378 , -6.828141 , -6.8359413, -6.8379807,\n",
              "       -6.8124194, -6.8204846, -6.83514  , -6.8771687, -6.8115625,\n",
              "       -6.80627  , -6.8042088, -6.810461 , -6.81897  , -6.7797728,\n",
              "       -6.8172274, -6.831664 , -6.837389 , -6.846701 , -6.8555145,\n",
              "       -6.837566 , -6.86582  , -6.8559637, -6.847285 , -6.8567734,\n",
              "       -6.814789 , -6.844452 , -6.8607316, -6.850774 , -6.8668365,\n",
              "       -6.893804 , -6.874565 , -6.866338 , -6.893798 , -6.8802567,\n",
              "       -6.860512 , -6.881175 , -6.885625 , -6.8376255, -6.839115 ,\n",
              "       -6.8622684, -6.8694916, -6.8136597, -6.8456182, -6.88194  ,\n",
              "       -6.8547397, -6.8688803, -6.8843174, -6.8765125, -6.883712 ,\n",
              "       -6.8481283, -6.850874 , -6.855592 , -6.8333426, -6.821801 ,\n",
              "       -6.792569 , -6.811334 , -6.79803  , -6.7925596, -6.7948647,\n",
              "       -6.791742 , -6.8118515, -6.833867 , -6.844953 , -6.8562956,\n",
              "       -6.850803 , -6.868211 , -6.856111 , -6.852242 , -6.858617 ,\n",
              "       -6.8494134, -6.8216286, -6.839472 , -6.8094225, -6.785757 ,\n",
              "       -6.8100915, -6.7888026, -6.771131 , -6.797405 , -6.798974 ,\n",
              "       -6.8020763, -6.8167615, -6.794673 , -6.8110476, -6.815769 ,\n",
              "       -6.833273 , -6.8448954, -6.833053 , -6.847465 , -6.84581  ,\n",
              "       -6.8489285, -6.841622 , -6.8243885, -6.8212056, -6.820993 ,\n",
              "       -6.8099384, -6.8180056, -6.821505 , -6.808032 , -6.8410525,\n",
              "       -6.846192 , -6.8499646, -6.8541822, -6.85507  , -6.8573904,\n",
              "       -6.8639326, -6.8661876, -6.8581963, -6.856992 , -6.845518 ,\n",
              "       -6.8378086, -6.830401 , -6.8422127, -6.7997603, -6.81553  ,\n",
              "       -6.8116827, -6.7981386, -6.8185234, -6.827529 , -6.826756 ,\n",
              "       -6.859711 , -6.8566494, -6.8474035, -6.8518944, -6.853066 ,\n",
              "       -6.854267 , -6.8525534, -6.9059057, -6.8540707, -6.8183255,\n",
              "       -6.852317 , -6.824561 , -6.834565 , -6.871435 , -6.8379583,\n",
              "       -6.810804 , -6.8238964, -6.8105097, -6.808707 , -6.7920547,\n",
              "       -6.7857146, -6.815107 , -6.7963343, -6.814032 , -6.8304987,\n",
              "       -6.8178844, -6.8339005, -6.848664 , -6.8365707, -6.8513   ,\n",
              "       -6.869733 , -6.8517084, -6.8509264, -6.845194 , -6.819871 ,\n",
              "       -6.815998 , -6.8070593, -6.8025618, -6.8198743, -6.8146806,\n",
              "       -6.821604 , -6.8328414, -6.833531 , -6.817486 , -6.826071 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8svTuxfuMwhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "end_logits_context = end_logits.numpy()[0, question_tok_len + 1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z2gGiOsfk40",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7f4d7ee2-0149-44a3-8e70-d3fe5c483f03"
      },
      "source": [
        "end_logits_context"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-6.7446074, -6.515649 , -6.458913 , -6.4504395, -6.3032613,\n",
              "       -5.0494056, -6.626792 , -6.5562367, -5.7418675, -6.692408 ,\n",
              "       -5.0974126, -6.119023 , -6.1501546, -5.0221753, -4.0058556,\n",
              "       -6.2092185, -6.435049 , -5.6676536, -3.2516017, -2.7163634,\n",
              "       -5.619785 , -5.8037176, -5.734294 , -5.5066605, -3.580192 ,\n",
              "       -6.1690755, -5.683391 , -6.221909 , -6.2033877, -5.412827 ,\n",
              "       -5.154177 , -4.993631 , -5.465799 , -3.9264193, -4.735207 ,\n",
              "       -5.657402 , -6.197018 , -5.514172 , -4.8591385, -6.355208 ,\n",
              "       -5.651754 , -4.0220866,  1.0050524, -3.05205  , -6.3515778,\n",
              "       -6.5526915, -6.441319 , -6.0244117, -6.791863 , -6.8768573,\n",
              "       -6.8974123, -6.3614044, -5.666736 , -6.0473213, -5.209497 ,\n",
              "       -6.550528 , -4.7900233, -6.344604 , -6.3565087, -6.376479 ,\n",
              "       -6.1835933, -6.237684 , -6.451015 , -6.1518364, -6.1440225,\n",
              "       -6.3175135, -6.254434 , -4.18049  , -4.5349164, -6.1386423,\n",
              "       -6.920312 , -6.169705 , -6.426493 , -6.183582 , -6.499347 ,\n",
              "       -5.11918  , -5.9656997, -6.5912347, -6.5363965, -4.312118 ,\n",
              "       -6.833821 , -6.632434 , -6.0380526, -6.616901 , -6.0614123,\n",
              "       -4.333576 , -6.040143 , -6.7738824, -6.7031326, -5.0526667,\n",
              "       -6.6215067, -6.6637797, -6.3891478, -6.2985435, -4.3804536,\n",
              "       -5.831202 , -6.80862  , -6.7925706, -6.1369557, -6.459629 ,\n",
              "       -5.625509 , -3.979258 , -6.060731 , -6.7819896, -6.6337614,\n",
              "       -6.5382733, -4.307201 , -6.5902843, -6.5851693, -5.799893 ,\n",
              "       -6.375116 , -6.3955274, -6.074672 , -4.979587 , -6.327733 ,\n",
              "       -6.3714576, -6.080155 , -3.9097252, -6.023023 , -6.3660035,\n",
              "       -6.350123 , -6.7437377, -6.453851 , -6.4579515, -5.2055697,\n",
              "       -6.0715957, -5.6625514, -6.0950117, -6.435264 , -6.0584702,\n",
              "       -6.1692176, -4.950303 , -5.9803977, -4.5692554, -5.6498103,\n",
              "       -6.490501 , -6.5715327, -6.5538173, -5.9503493, -5.412787 ,\n",
              "       -3.3055296, -6.3098392, -6.8140564, -6.8042364, -6.7874904,\n",
              "       -6.798538 , -6.823935 , -6.8048987, -6.811523 , -6.8067107,\n",
              "       -6.795523 , -6.801499 , -6.802662 , -6.801959 , -6.808463 ,\n",
              "       -6.8011117, -6.799064 , -6.800338 , -6.7984853, -6.786903 ,\n",
              "       -6.8044863, -6.795306 , -6.8053374, -6.8085904, -6.800196 ,\n",
              "       -6.809532 , -6.8157363, -6.804541 , -6.8021913, -6.7794766,\n",
              "       -6.764994 , -6.6215687, -6.710682 , -6.746304 , -6.762942 ,\n",
              "       -6.7749043, -6.794625 , -6.8080754, -6.794897 , -6.7934594,\n",
              "       -6.7984843, -6.783243 , -6.795158 , -6.795938 , -6.799563 ,\n",
              "       -6.771318 , -6.8088555, -6.8126745, -6.786438 , -6.7951016,\n",
              "       -6.7464128, -6.733177 , -6.723927 , -6.738104 , -6.7795606,\n",
              "       -6.7667727, -6.791945 , -6.7934184, -6.78628  , -6.790892 ,\n",
              "       -6.813499 , -6.7801404, -6.7874312, -6.8096557, -6.7290864,\n",
              "       -6.749592 , -6.7497854, -6.783298 , -6.789619 , -6.7652097,\n",
              "       -6.809296 , -6.8080416, -6.7877626, -6.796371 , -6.8055463,\n",
              "       -6.8169646, -6.7852244, -6.759398 , -6.7683325, -6.7762055,\n",
              "       -6.796741 , -6.7789927, -6.743818 , -6.7429953, -6.7796903,\n",
              "       -6.751065 , -6.797068 , -6.8114543, -6.7841873, -6.8148003,\n",
              "       -6.8211894, -6.796683 , -6.8113565, -6.8133607, -6.7869916,\n",
              "       -6.831644 , -6.8148646, -6.789892 , -6.806451 , -6.8076777,\n",
              "       -6.7901435, -6.8246045, -6.8256345, -6.790479 , -6.8067055,\n",
              "       -6.813291 , -6.8083143, -6.772675 , -6.704222 , -6.7859383,\n",
              "       -6.698653 , -6.697579 , -6.670986 , -6.7223477, -6.784555 ,\n",
              "       -6.781902 , -6.778665 , -6.8236365, -6.8057604, -6.770967 ,\n",
              "       -6.824706 , -6.7998385, -6.8107543, -6.8150477, -6.8011117,\n",
              "       -6.8319097, -6.7842383, -6.7893443, -6.7974772, -6.7736626,\n",
              "       -6.802944 , -6.833306 , -6.82479  , -6.8131037, -6.8287973,\n",
              "       -6.829602 , -6.7565417, -6.795622 , -6.828918 , -6.822665 ,\n",
              "       -6.836739 , -6.8417706, -6.7606683, -6.739113 , -6.822009 ,\n",
              "       -6.776514 , -6.7576966, -6.7908955, -6.793322 , -6.7949014,\n",
              "       -6.7862267, -6.7754993, -6.7842174, -6.8009324, -6.807339 ,\n",
              "       -6.8064713, -6.818133 , -6.7726846, -6.8129516, -6.827753 ,\n",
              "       -6.8140016, -6.8067055, -6.8405366, -6.835174 , -6.8254437,\n",
              "       -6.8323045, -6.7858396, -6.7845726, -6.826346 , -6.8209844,\n",
              "       -6.820193 , -6.815339 , -6.7969685, -6.8142724, -6.8107777,\n",
              "       -6.771524 , -6.8164744, -6.824636 , -6.801144 , -6.8249965,\n",
              "       -6.820874 , -6.798954 , -6.8044233, -6.8189993, -6.820295 ,\n",
              "       -6.815723 , -6.814014 , -6.8029857, -6.819141 , -6.829823 ,\n",
              "       -6.8092527, -6.816052 , -6.822759 , -6.828361 , -6.8216653,\n",
              "       -6.783482 , -6.806777 , -6.768674 , -6.799834 , -6.787069 ,\n",
              "       -6.7517323, -6.7570367, -6.767039 , -6.7806287, -6.7958794,\n",
              "       -6.7986174, -6.8056207, -6.8126283, -6.7778134, -6.8051414,\n",
              "       -6.820762 , -6.796069 , -6.8004546, -6.822145 , -6.8205943,\n",
              "       -6.8329706, -6.8330245, -6.8179564, -6.8399715, -6.828831 ,\n",
              "       -6.815327 , -6.818216 , -6.822882 , -6.831307 , -6.8356295,\n",
              "       -6.7813993, -6.7759624, -6.8127966, -6.797126 , -6.800291 ,\n",
              "       -6.8135324, -6.80279  , -6.814205 , -6.8010783, -6.824013 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWPZIYCInv1_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "2ba703b0-ee8a-4ba3-a875-a9b3594ce921"
      },
      "source": [
        "print(context_tok_to_word_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 18, 19, 20, 20, 20, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 33, 34, 35, 36, 37, 37, 38, 39, 40, 41, 42, 43, 44, 44, 44, 45, 46, 47, 48, 49, 49, 49, 49, 50, 51, 52, 53, 54, 55, 56, 56, 56, 57, 58, 58, 58, 59, 60, 61, 62, 63, 64, 65, 66, 66, 67, 68, 69, 69, 70, 71, 72, 73, 74, 75, 76, 77, 77, 78, 79, 80, 81, 82, 83, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 113, 114, 115, 116, 117, 118, 118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNxJEHKhM1BF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a2d6ec95-b453-4d57-828c-54e054f3fbbe"
      },
      "source": [
        "np.argmax(start_logits_context)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb7BpMlUM5t0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "06e99d8e-cb53-4e65-cc74-cd7a859fbeae"
      },
      "source": [
        "start_word_id = context_tok_to_word_id[np.argmax(start_logits_context)]\n",
        "start_word_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1bRnIBifL60",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2e379d0e-4947-4c4f-aa4c-0608e0f2f32f"
      },
      "source": [
        "end_word_id = context_tok_to_word_id[np.argmax(end_logits_context)]\n",
        "end_word_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v28pjXleiCu-",
        "colab_type": "text"
      },
      "source": [
        "RESPOSTA FINAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw7V2zJFfoUI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "59a2e347-2a9c-4b8d-ff03-d62e2eef5b6c"
      },
      "source": [
        "predicted_answer = \" \".join(my_context_words[start_word_id:end_word_id + 1])\n",
        "print(\"The answer to:\\n\" + my_question + \"\\nis:\\n\" + predicted_answer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The answer to:\n",
            "What is civil disobedience?\n",
            "is:\n",
            "one of the many ways people have rebelled against what they deem to be unfair laws.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIrirtu4e75H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "33a08c02-e78a-4053-e902-f59a5d4b41ac"
      },
      "source": [
        "from IPython.core.display import HTML\n",
        "display(HTML(f'<h2>{my_question.upper()}</h2>'))\n",
        "marked_text = str(my_context.replace(predicted_answer, f\"<mark>{predicted_answer}</mark>\"))\n",
        "#print(marked_text)\n",
        "display(HTML(f\"\"\"<blockquote> {marked_text} </blockquote>\"\"\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h2>WHAT IS CIVIL DISOBEDIENCE?</h2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<blockquote> One of its earliest massive implementations was brought about by Egyptians against the British occupation in the 1919 Revolution. Civil disobedience is <mark>one of the many ways people have rebelled against what they deem to be unfair laws.</mark> It has been used in many nonviolent resistance movements in India (Gandhi's campaigns for independence from the British Empire), in Czechoslovakia's Velvet Revolution and in East Germany to oust their communist governments, In South Africa in the fight against apartheid, in the American Civil Rights Movement, in the Singing Revolution to bring independence to the Baltic countries from the Soviet Union, recently with the 2003 Rose Revolution in Georgia and the 2004 Orange Revolution in Ukraine, among other various movements worldwide. </blockquote>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHH3q0MQfONh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}